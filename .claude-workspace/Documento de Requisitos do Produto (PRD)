

Documento de Requisitos do Produto (PRD) detalha a modernização do site e do chatbot de Roteiro de Dispensação, utilizando a metodologia PREVC. O projeto visa melhorar a usabilidade e o desempenho, adicionando um chatbot moderno com suporte a duas personas (técnica e leiga) baseado em RAG com LangFlow e Kimie K2 via OpenRouter, hospedado no Render com CI/CD. # Documento de Requisitos do Produto (PRD): Modernização do Site e Chatbot de Roteiro de Dispensação ## 1. Introdução ### 1.1. Objetivo do Documento Este Documento de Requisitos do Produto (PRD) tem como objetivo principal **definir os requisitos, a arquitetura, a metodologia de desenvolvimento e os critérios de aceitação para a modernização do site e do chatbot de Roteiro de Dispensação**. O documento servirá como guia para a equipe de desenvolvimento e como base para a comunicação com os stakeholders, garantindo que o produto final atenda às expectativas e necessidades do público-alvo. Ele detalha as fases de Planejamento, Revisão, Execução, Validação e Confirmação (PREVC), as tecnologias a serem utilizadas e os critérios para a entrega bem-sucedida do projeto. ### 1.2. Escopo do Projeto O escopo deste projeto abrange a **revisão e atualização do site existente, atualmente hospedado na plataforma Render, e a evolução significativa do chatbot integrado**. As principais atividades incluem: * **Modernização do Site**: Implementação de CI/CD via `render.yaml`, refatoração da aplicação Python com Gunicorn, e utilização de Secret Files para gerenciamento seguro de variáveis de ambiente. * **Evolução do Chatbot**: * Suporte a **duas personas distintas**: uma técnica (Dr. Gasnelio, anteriormente "Professor") e outra leiga (Gá, o "Amigo"). * Base de conhecimento proveniente de uma tese de doutorado, processada com **arquitetura RAG (Retrieval-Augmented Generation)**. * Implementação do RAG utilizando **LangFlow**, com embeddings do Hugging Face e vetorização FAISS ou Astra DB. * Integração de uma nova camada de backend com o modelo de linguagem **Kimie K2 Free**, acessado via **OpenRouter**. * **Infraestrutura e Automação**: Análise do projeto atual, criação de um novo repositório (`roteiro-de-dispersacao-v4`), clonagem seletiva de arquivos essenciais, configuração de ambiente no Render, pipeline CI/CD acionada por `git push`, e criação de serviços Blueprint no Render. * **Fluxo de Desenvolvimento e Integração LangFlow**: Detalhamento da arquitetura RAG (MarkdownLoader, Splitter, Embeddings, Banco de Dados Vetorial, LLM), montagem do fluxo no LangFlow e publicação como API, e uso de `PromptTemplate` com injeção dinâmica de persona via backend. O projeto será desenvolvido com a **premissa de custo zero**, utilizando ferramentas e serviços gratuitos sempre que possível. O objetivo principal é melhorar a usabilidade, otimizar o desempenho e adicionar um chatbot mais moderno e eficaz. ### 1.3. Definições, Acrônimos e Abreviações * **PRD**: Documento de Requisitos do Produto (Product Requirements Document). * **PREVC**: Metodologia de desenvolvimento composta por Planejamento (P), Revisão (R), Execução (E), Validação (V) e Confirmação (C). * **CI/CD**: Integração Contínua/Entrega Contínua (Continuous Integration/Continuous Delivery). * **RAG**: Geração Aumentada por Recuperação (Retrieval-Augmented Generation). * **LLM**: Modelo de Linguagem Grande (Large Language Model). * **API**: Interface de Programação de Aplicações (Application Programming Interface). * **FAISS**: Facebook AI Similarity Search (Biblioteca para busca de similaridade em vetores). * **Astra DB**: Banco de dados vetorial gerenciado pela DataStax. * **Render**: Plataforma de nuvem para hospedagem de aplicações web e serviços. * **LangFlow**: Ferramenta de interface gráfica para construção de fluxos de LangChain. * **OpenRouter**: Plataforma que fornece acesso unificado a diversos LLMs via API. * **Kimie K2 (Kimi K2)**: Modelo de linguagem grande desenvolvido pela Moonshot AI. * **Hugging Face**: Plataforma e comunidade de machine learning focado em NLP e modelos de transformadores. * **Streamlit**: Biblioteca Python para criação de aplicações web interativas. * **Gunicorn**: Servidor WSGI HTTP para aplicações Python. * **WSGI**: Web Server Gateway Interface (Interface de Porta de Entrada para Servidor Web). * **MoE**: Mixture-of-Experts (Arquitetura de modelo de linguagem). ## 2. Metodologia PREVC A **metodologia PREVC (Planejamento, Revisão, Execução, Validação e Confirmação)** será adotada para estruturar o desenvolvimento e a modernização do site e do chatbot. Esta abordagem visa garantir um ciclo de vida de projeto organizado e iterativo, assegurando que todos os aspectos do produto sejam considerados e validados em cada etapa. O **Planejamento (P)** envolve a definição clara do escopo, objetivos, tecnologias, metas e personas. A **Revisão (R)** consiste na análise técnica do ambiente atual, testes de fluxos e revisão da base de conhecimento. A **Execução (E)** abrange a implementação prática, incluindo a implantação CI/CD, upload da base de conhecimento e criação de rotas de backend. A **Validação (V)** foca em testes de funcionalidade, usabilidade e coerência dos resultados. Por fim, a **Confirmação (C)** é a etapa final de verificação do sistema em produção, segurança dos dados e documentação de entrega. Esta estrutura cíclica permite ajustes contínuos e assegura a entrega de um produto que atenda às expectativas dos stakeholders e às necessidades do público-alvo. ### 2.1. Planejamento (P) A fase de **Planejamento (P)** da metodologia PREVC concentra-se na **definição clara dos objetivos, escopo, tecnologias, metas, stakeholders, público-alvo, personas do chatbot, requisitos funcionais e não-funcionais, arquitetura proposta e ferramentas a serem utilizadas**. Esta etapa é fundamental para estabelecer uma base sólida para o projeto, alinhando expectativas e garantindo que todos os envolvidos tenham uma compreensão comum do que será desenvolvido e como será realizado. O planejamento detalhado permite antecipar desafios, definir métricas de sucesso e estruturar as fases subsequentes de Revisão, Execução, Validação e Confirmação. A Seção 3 deste documento é dedicada a detalhar os elementos do Planejamento. ### 2.2. Revisão (R) A fase de **Revisão (R)** da metodologia PREVC concentra-se na **análise detalhada do estado atual do projeto e dos componentes que serão modernizados**. Inicialmente, foi realizada uma tentativa de análise do site existente, hospedado na plataforma Render (<https://repositorio-roteiro-de-dispersacao.onrender.com/>), e do seu respectivo repositório GitHub (<https://github.com/AnalineS/repositorio_roteiro_de_dispersacao>). No entanto, a busca pelo repositório GitHub não retornou resultados, indicando que o repositório pode ser privado ou o URL fornecido esteja incorreto . A visita ao site permitiu identificar seu título como "Tese de Doutorado - Roteiro de Dispensação para Hanseníase" e observar a presença de links de navegação como "Início", "Sobre", "A Tese", "Chatbot" e "Contato", além de botões para "Leia a Tese Completa" e "Conversar com a IA" . Tentativas subsequentes de interagir com o elemento "Chatbot" ou "Conversar com a IA" para entender melhor a implementação atual do chatbot não foram bem-sucedidas devido a erros técnicos nas ferramentas de análise , . Paralelamente à análise do site e repositório, a fase de Revisão incluiu uma investigação sobre as tecnologias e arquiteturas propostas para a modernização. Foram realizadas buscas por diagramas de arquitetura que ilustrassem a integração de um sistema RAG (Retrieval-Augmented Generation) utilizando LangFlow, com opções de vetorização FAISS ou Astra DB, e a utilização do modelo Kimie K2 Free via OpenRouter, hospedado em Render. Embora buscas diretas por diagramas específicos para essa combinação exata de tecnologias não tenham sido totalmente satisfatórias, a pesquisa forneceu insights valiosos. Por exemplo, um artigo no Medium detalhou a construção de um assistente de IA com LangFlow e AstraDB, descrevendo o fluxo de dados para transformar novos dados em embeddings e armazená-los, bem como o processo de consulta semântica ao banco vetorial . Outro artigo da Data Science Academy explicou conceitualmente o funcionamento do RAG, destacando a criação de dados externos, recuperação de informações relevantes e enriquecimento de prompts fornecidos ao LLM . Essas fontes ajudaram a compreender os componentes e fluxos gerais de um sistema RAG. A investigação sobre a plataforma Render, onde o site será hospedado, também fez parte desta fase. Buscas por "arquitetura da plataforma Render" ou "diagrama de Render" não resultaram em diagramas específicos da infraestrutura do Render, mas trouxeram informações sobre serviços de renderização 3D e arquitetura de software, que não eram o foco principal , . Uma busca mais direcionada para "arquitetura de serviço e comunicação na plataforma Render" também não forneceu diagramas detalhados, mas trouxe links sobre arquitetura lógica de sistemas e arquitetura de serviço do Google Cloud Deploy, que, embora não sejam específicos do Render, oferecem insights sobre conceitos gerais de arquitetura de nuvem e CI/CD , . Uma avaliação do serviço Render em um site de reviews mencionou suas funcionalidades robustas, a utilidade do SSL automático, mas também apontou para a possibilidade de custos não explicitamente comunicados e a necessidade de melhor documentação para implantação de aplicativos . Aprofundando a revisão da arquitetura do chatbot, buscas foram realizadas para entender a implementação de chatbots com múltiplas personas, especificamente "chatbot com duas personas python arquitetura". Um artigo do NIH forneceu uma visão geral das tecnologias e arquiteturas de chatbots, incluindo componentes como NLP, gestão de diálogo e integração, e mencionou o uso de Python como linguagem de desenvolvimento . Outro artigo, mais recente e específico, descreveu uma análise técnica da arquitetura RAG de um chatbot chamado "Juci", detalhando a ingestão e manutenção incremental da base de conhecimento usando Qdrant como banco de dados vetorial e o pipeline RAG em execução com um servidor TCP em Python . Essa análise mostrou como consultas são processadas, realizando buscas de similaridade paralelas em diferentes coleções do banco vetorial antes de enviar o prompt ao LLM. Considerando a opção de usar FAISS para vetorização, buscas como "FAISS dentro do pipeline de chatbot" revelaram projetos e tutoriais relevantes. Um repositório GitHub "DeepSeek RAG Chatbot" demonstrou uma pilha RAG usando FAISS, GraphRAG, e integração de histórico de chat, com opções de instalação tradicional (Python/venv) ou Docker . Um tutorial no Medium detalhou a construção de um chatbot RAG em Python usando Hugging Face Transformers e FAISS, incluindo trechos de código para embedar documentos, construir o índice FAISS e criar funções de recuperação . Outro repositório GitHub, "PDF-AnswerHub-GenAI-ChatBot", apresentou um chatbot RAG usando FAISS, LangChain e Streamlit, focando na extração de informações de documentos PDF . Esses exemplos práticos forneceram uma compreensão mais concreta de como o FAISS pode ser integrado em um pipeline RAG com Python, muitas vezes em conjunto com bibliotecas como LangChain ou Transformers da Hugging Face. A visita ao repositório "PDF-AnswerHub-GenAI-ChatBot" confirmou o uso de LangChain para gerenciar embeddings e interações com o modelo de linguagem, além de FAISS para busca de similaridade e Streamlit para a interface do usuário . Essas informações são cruciais para a fase de Execução, pois LangFlow, tecnologia escolhida para o projeto, é construído sobre o LangChain. ### 2.3. Execução (E) A fase de **Execução (E)** da metodologia PREVC é onde ocorre a **implementação prática de todos os componentes planejados e revisados**. Esta etapa envolve a configuração do ambiente e da infraestrutura, o desenvolvimento e a integração do chatbot, o desenvolvimento e a atualização do site, a implementação da pipeline de CI/CD e a integração com serviços externos como OpenRouter e Kimie K2. A execução deve seguir o plano estabelecido, utilizando as tecnologias e ferramentas selecionadas, e garantindo a qualidade do código e a aderência aos requisitos. A Seção 5 deste documento detalha as atividades e considerações da fase de Execução. ### 2.4. Validação (V) A fase de **Validação (V)** da metodologia PREVC tem como objetivo **assegurar que o produto desenvolvido atenda aos requisitos especificados e às expectativas dos stakeholders**. Esta etapa envolve a realização de testes de funcionalidade, testes de usabilidade e a verificação da coerência das personas do chatbot. Os critérios de aceitação definidos para o site e para o chatbot serão utilizados para avaliar se o produto está pronto para a fase de Confirmação. A Seção 6 deste documento descreve os critérios de aceitação e os tipos de testes a serem realizados durante a Validação. ### 2.5. Confirmação (C) A fase de **Confirmação (C)** da metodologia PREVC representa a **etapa final do ciclo de desenvolvimento, focando na verificação final do sistema em produção, na revisão de segurança dos dados e na documentação de entrega**. É o momento de garantir que o produto está totalmente funcional, seguro e pronto para ser utilizado pelo público-alvo. Esta fase também inclui a preparação para a escalabilidade do sistema, considerando futuras expansões ou aumento de carga. A Seção 7 deste documento detalha as atividades e entregáveis da fase de Confirmação. ## 3. Planejamento (P) ### 3.1. Objetivos e Metas O **objetivo principal** deste projeto é a **modernização do site e do chatbot de Roteiro de Dispensação**, visando **melhorar significativamente a usabilidade, otimizar o desempenho e introduzir um novo chatbot mais moderno e eficaz**. Este chatbot deve ser capaz de atender a duas personas distintas: uma técnica e outra leiga, utilizando uma base de conhecimento específica (uma tese de doutorado) por meio da arquitetura RAG. **Metas específicas** incluem: * **Atualização Tecnológica do Site**: Migrar para uma arquitetura mais robusta e escalável, utilizando Python com Gunicorn no Render, com CI/CD automatizada via `render.yaml` e gerenciamento seguro de configurações. * **Implementação do Chatbot com Duas Personas**: * **Persona Técnica (Dr. Gasnelio)**: Deve responder perguntas técnicas sobre a tese de forma culta e profissional, como um professor de mestrado. * **Persona Leiga (Gá)**: Deve se apresentar como um amigo, utilizando linguagem cotidiana, bem-humorada e empática, explicando conceitos complexos de forma simplificada, acessível até para crianças ou pessoas sem formação acadêmica. * **Integração de RAG com LangFlow e Kimie K2**: Desenvolver um pipeline RAG eficiente utilizando LangFlow para gerenciar a base de conhecimento (embeddings via Hugging Face, vetorização com FAISS ou Astra DB) e integrar o modelo de linguagem Kimie K2 Free via OpenRouter para a geração de respostas. * **Custo Zero**: Realizar o projeto utilizando exclusivamente ferramentas e serviços gratuitos, dentro dos limites de suas camadas free. * **Preparação para Expansão**: Estruturar o site e o chatbot de forma a facilitar a futura inclusão de informações sobre outras doenças, além da hanseníase, contempladas no roteiro. * **Entrega em Prazo Razoável**: Embora não haja um prazo rígido, a expectativa é que um site de poucas páginas e um chatbot funcional sejam entregues em um período não superior a um mês. ### 3.2. Stakeholders e Expectativas Os principais **stakeholders** deste projeto e suas respectivas expectativas são: | Stakeholder | Expectativas | Interesse no Projeto | |---------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------| | **Estudantes de Doutorado** | Acesso a informações técnicas detalhadas da tese de forma fácil e rápida; ferramenta de apoio para pesquisas e estudos. | Alto; utilização direta da base de conhecimento e das funcionalidades técnicas. | | **Professores da UNB** | Disseminação do conhecimento produzido na tese; ferramenta de apoio ao ensino e à orientação de alunos; visibilidade do trabalho acadêmico. | Alto; validação e aplicação prática do conhecimento gerado. | | **Autores da Tese** | Amplo alcance e impacto da pesquisa; reconhecimento do trabalho desenvolvido; ferramenta útil para a comunidade. | Muito Alto; o projeto é uma extensão direta de seu trabalho acadêmico. | | **Pacientes e Familiares** | Acesso a informações claras, simples e confiáveis sobre a doença e o roteiro de dispensação; suporte e orientação de forma acessível. | Médio a Alto; benefício direto com a persona leiga do chatbot. | | **Comunidade Acadêmica (Geral)** | Acesso a um repositório de conhecimento especializado e a uma ferramenta de IA aplicada à saúde. | Médio; interesse em novas aplicações de IA e em conteúdo de pesquisa. | | **Equipe de Desenvolvimento** | Sucesso do projeto; aprendizado e aplicação de novas tecnologias; entrega dentro do escopo, prazo e orçamento (gratuito). | Alto; responsabilidade direta pela execução e qualidade do produto. | A expectativa geral é que o site e o chatbot modernizados se tornem uma **ferramenta valiosa para a disseminação de conhecimento sobre o roteiro de dispensação**, atendendo a um público diversificado e contribuindo para a prática da farmácia clínica. ### 3.3. Público-Alvo O **público-alvo** do site e do chatbot modernizados é amplo e diversificado, refletindo os próprios stakeholders e suas necessidades de informação. Não há limitações rígidas quanto a idade, gênero, classe social ou formação acadêmica. No entanto, inicialmente, o foco geográfico pode ser em **Brasília**, devido à origem universitária do projeto (UNB). Os principais segmentos do público-alvo incluem: * **Acadêmicos e Pesquisadores**: Estudantes de graduação, pós-graduação (especialmente doutorado), professores e pesquisadores das áreas de saúde, farmácia e ciências afins, que buscam informações técnicas e detalhadas sobre o roteiro de dispensação e a tese que o fundamenta. * **Profissionais de Saúde**: Farmacêuticos clínicos, médicos, enfermeiros e outros profissionais de saúde que atuam na linha de frente do cuidado ao paciente e podem se beneficiar de informações sobre o roteiro de dispensação para hanseníase e, futuramente, outras doenças. * **Pacientes e seus Familiares**: Indivíduos afetados pela hanseníase (e posteriormente outras doenças) e seus cuidadores, que necessitam de informações claras, compreensíveis e empáticas sobre a doença, o tratamento e o roteiro de dispensação. Este grupo se beneficiará especialmente da persona leiga do chatbot ("Gá"). * **Gestores e Formuladores de Políticas Públicas**: Profissionais envolvidos na gestão de saúde pública e na elaboração de políticas relacionadas ao fornecimento de medicamentos e ao cuidado de doenças específicas. * **Público em Geral Interessado em Saúde**: Qualquer pessoa que tenha interesse em aprender mais sobre hanseníase, roteiros de dispensação ou o uso de IA na área da saúde. A capacidade do chatbot de se comunicar através de duas personas distintas visa justamente atender a essa diversidade de público, oferecendo tanto profundidade técnica quanto simplicidade e empatia, conforme a necessidade do usuário. ### 3.4. Personas do Chatbot O chatbot modernizado deverá suportar **duas personas distintas**, permitindo que os usuários escolham o estilo de interação mais adequado às suas necessidades e nível de conhecimento. A definição clara dessas personas é crucial para orientar o desenvolvimento da lógica de geração de respostas e a construção dos `PromptTemplates`. 1. **Dr. Gasnelio (Persona Técnica - anteriormente "Professor")**: * **Perfil**: Especialista em farmácia clínica, com foco em roteiros de dispensação. Apresenta-se como um professor ou pesquisador de mestrado/doutorado. * **Objetivo**: Fornecer informações técnicas, precisas e detalhadas extraídas da tese de doutorado. Auxiliar em pesquisas acadêmicas e na compreensão profunda dos conceitos. * **Tom de Voz**: Culto, profissional, objetivo, direto e formal. * **Linguagem**: Utiliza terminologia técnica e científica adequada. As respostas devem ser estruturadas e fundamentadas. * **Exemplo de Saudação**: "Saudações! Sou o Dr. Gasnelio. Minha pesquisa foca no roteiro de dispensação para a prática da farmácia clínica. Como posso auxiliá-lo hoje com informações técnicas?" . * **Público-Alvo Primário**: Estudantes de doutorado, professores, pesquisadores, profissionais de saúde com formação técnica. 2. **Gá (Persona Leiga - "Amigo")**: * **Perfil**: Amigável, empático, paciente e bem-humorado. Apresenta-se como um amigo que explica coisas complexas de forma simples. * **Objetivo**: Explicar conceitos relacionados à hanseníase e ao roteiro de dispensação de forma acessível, clara e compreensível para um público leigo. Oferecer suporte e orientação de maneira acolhedora. * **Tom de Voz**: Descontraído, caloroso, encorajador, solidário e informal (mas sempre respeitoso). * **Linguagem**: Utiliza linguagem cotidiana, evita jargões técnicos ou os explica de forma simples. Pode usar analogias e exemplos concretos. As respostas devem ser curtas, diretas e focadas na compreensão do usuário. * **Exemplo de Saudação**: "Oi! Sou o Gá, seu amigo virtual para tirar dúvidas sobre saúde. Vou tentar explicar as coisas de um jeito bem fácil, tá bom? O que você gostaria de saber?" * **Público-Alvo Primário**: Pacientes, familiares de pacientes, público em geral sem formação específica na área, crianças (com supervisão). A **injeção dinâmica de `PromptTemplates`** no backend será a técnica utilizada para orientar o LLM (Kimie K2) a adotar o comportamento e o estilo de linguagem da persona selecionada pelo usuário. Esses templates conterão instruções contextuais específicas para cada persona, que serão combinadas com a pergunta do usuário e os trechos relevantes da tese recuperados pelo RAG. ### 3.5. Requisitos Funcionais Os requisitos funcionais descrevem as funcionalidades que o sistema modernizado deve oferecer. **Para o Site:** 1. **RF-S01: Exibição de Conteúdo Estático**: O site deve exibir páginas com informações sobre a tese, o projeto, os autores e contatos. 2. **RF-S02: Navegação Intuitiva**: Deve oferecer uma estrutura de navegação clara e de fácil uso entre as diferentes seções do site. 3. **RF-S03: Integração com Chatbot**: Deve incorporar a interface do chatbot de forma fluida e responsiva. 4. **RF-S04: Responsividade**: O site deve ser acessível e adaptar seu layout a diferentes tamanhos de tela (desktop, tablet, mobile). 5. **RF-S05: Link para Tese Completa**: Deve fornecer um link ou mecanismo para acesso à tese de doutorado completa. **Para o Chatbot:** 1. **RF-C01: Seleção de Persona**: O usuário deve poder escolher entre as personas "Dr. Gasnelio" (técnica) e "Gá" (leiga) antes ou durante a interação. 2. **RF-C02: Interação via Chat**: O usuário deve poder digitar perguntas e receber respostas do chatbot em formato de conversa. 3. **RF-C03: Respostas Baseadas em RAG**: As respostas do chatbot devem ser geradas com base na recuperação de informações da tese de doutorado, utilizando a arquitetura RAG. 4. **RF-C04: Adaptação de Linguagem**: O chatbot deve adaptar seu estilo de linguagem e nível de detalhe de acordo com a persona selecionada. * **RF-C04.1 (Dr. Gasnelio)**: Linguagem técnica, culta e profissional. * **RF-C04.2 (Gá)**: Linguagem simples, cotidiana, empática e bem-humorada. 5. **RF-C05: Gerenciamento de Contexto (Opcional)**: O chatbot deve ser capaz de manter o contexto da conversa por um curto período ou um número limitado de interações, se viável com as tecnologias escolhidas. 6. **RF-C06: Indicação de Fontes (Opcional)**: Se possível, o chatbot deve indicar as seções ou páginas da tese que foram utilizadas para gerar a resposta. **Para o Backend e Infraestrutura:** 1. **RF-B01: CI/CD Automatizada**: O sistema deve ser implantado no Render utilizando um pipeline de CI/CD configurado via `render.yaml`, acionado por `git push`. 2. **RF-B02: Gerenciamento de Secrets**: Chaves de API e outras informações sensíveis devem ser armazenadas e gerenciadas de forma segura utilizando Secret Files do Render. 3. **RF-B03: Integração com OpenRouter**: O backend deve se comunicar com a API da OpenRouter para acessar o modelo Kimie K2. 4. **RF-B04: Processamento RAG**: O backend (via LangFlow) deve ser capaz de carregar a tese, gerar embeddings, armazenar no banco de dados vetorial e recuperar informações relevantes para as perguntas. 5. **RF-B05: Injeção Dinâmica de Persona**: O backend deve ser responsável por injetar dinamicamente as instruções da persona selecionada no `PromptTemplate` antes de enviar a solicitação ao LLM. ### 3.6. Requisitos Não-Funcionais Os requisitos não-funcionais definem as propriedades e qualidades do sistema. 1. **RNF-01: Desempenho (Latência)**: O tempo de resposta do chatbot (desde a pergunta do usuário até a exibição da resposta) deve ser aceitável, idealmente abaixo de 5-10 segundos para a maioria das interações, considerando as limitações das ferramentas gratuitas. 2. **RNF-02: Disponibilidade**: O site e o chatbot devem estar disponíveis na maior parte do tempo, dentro dos limites do plano gratuito do Render (que pode ter períodos de inatividade ou sleep). 3. **RNF-03: Usabilidade**: O site e a interface do chatbot devem ser intuitivos e de fácil uso para o público-alvo definido, mesmo para usuários leigos. 4. **RNF-04: Segurança de Dados**: As informações sensíveis, como chaves de API, devem ser protegidas. A base de conhecimento (tese) é pública, mas o sistema não deve expor dados internos ou de configuração. 5. **RNF-05: Custo Zero**: O projeto deve ser desenvolvido e mantido sem custos financeiros, utilizando ferramentas e serviços em suas camadas gratuitas. 6. **RNF-06: Escalabilidade (Limitada)**: O sistema deve ser projetado considerando as limitações de escalabilidade dos planos gratuitos, mas com uma arquitetura que permita uma futura evolução para planos pagos se necessário. 7. **RNF-07: Manutenibilidade**: O código-fonte e a configuração do sistema devem ser organizados de forma clara para facilitar a manutenção e futuras atualizações. 8. **RNF-08: Acessibilidade (Básica)**: O site e o chatbot devem seguir práticas básicas de acessibilidade web (ex.: contraste de cores, tamanho de fonte, labels para elementos interativos). 9. **RNF-09: Precisão das Respostas**: As respostas do chatbot devem ser precisas e baseadas estritamente no conteúdo da tese de doutorado, minimizando alucinações do LLM através do uso eficaz do RAG. 10. **RNF-10: Coerência das Personas**: As respostas geradas para cada persona devem ser consistentes com suas características definidas (tom de voz, linguagem, nível de detalhe). ### 3.7. Arquitetura Proposta A arquitetura proposta para a modernização do site e do chatbot visa criar um sistema robusto, escalável e de baixo custo (gratuito, conforme a premissa do projeto), alavancando tecnologias modernas e práticas de desenvolvimento ágil. O sistema será composto por uma aplicação web Python hospedada na plataforma Render, integrada a um chatbot avançado baseado em RAG (Retrieval-Augmented Generation) e um modelo de linguagem moderno. A comunicação entre os componentes e a gestão do fluxo de dados serão fundamentais para o funcionamento eficiente do sistema. **Componentes Principais da Arquitetura:** 1. **Aplicação Web Python (Render):** * **Hospedagem**: A aplicação web principal, responsável pelo frontend do site e pelo backend que intermediará as requisições do chatbot, será desenvolvida em Python e hospedada na plataforma Render. O uso de gunicorn como servidor WSGI garantirá a capacidade de lidar com múltiplas requisições simultaneamente. * **Variáveis de Ambiente e Segurança**: A configuração do ambiente será gerenciada através de variáveis de ambiente no Render, com dados sensíveis, como chaves de API e credenciais de banco de dados, armazenados de forma segura utilizando Secret Files do Render. * **CI/CD**: A implantação contínua será configurada utilizando um arquivo `render.yaml`, que definirá os serviços e o pipeline de CI/CD. O gatilho para novas implantações será o `git push` para o repositório principal, agilizando o processo de atualização do site. * **Blueprints no Render**: A organização dos serviços na Render poderá utilizar Blueprints para gerenciar múltiplos serviços de forma coordenada, se aplicável ao escopo final. 2. **Sistema de Chatbot (RAG com LangFlow):** * **Base de Conhecimento**: O cerne do chatbot será uma base de conhecimento derivada de uma tese de doutorado. Esta base será processada e disponibilizada para o sistema RAG. * **Arquitetura RAG**: O fluxo RAG será implementado utilizando o LangFlow, uma ferramenta que permite a construção visual de cadeias de LangChain. * **Ingestão de Dados**: A tese de doutorado, provavelmente em formato Markdown ou PDF, será carregada utilizando componentes como `MarkdownLoader` ou `PyPDFLoader` dentro do LangFlow. * **Processamento de Texto**: O texto carregado será dividido em "chunks" (pedaços menores e gerenciáveis) utilizando um `TextSplitter` (ex: `RecursiveCharacterTextSplitter`) para facilitar a criação de embeddings e a recuperação eficiente. * **Embeddings**: Os chunks de texto serão convertidos em representações vetoriais (embeddings) utilizando modelos da Hugging Face (ex: `sentence-transformers/all-MiniLM-L6-v2` como sugerido em exemplos práticos ). Esses embeddings capturam o significado semântico do texto. * **Armazenamento Vetorial (FAISS ou Astra DB)**: Os embeddings gerados serão armazenados em um banco de dados vetorial. * **FAISS**: Uma opção local e eficiente, adequada para o plano gratuito do Render. O índice FAISS será construído e persistido, permitindo buscas rápidas por similaridade. Projetos como o "DeepSeek RAG Chatbot" e tutoriais demonstram sua integração em pipelines RAG com Python. * **Astra DB**: Uma opção de banco de dados vetorial escalável e gerenciada, baseada no Apache Cassandra e oferecida pela DataStax. Pode ser considerada se houver necessidade de maior escalabilidade ou funcionalidades gerenciadas, embora possa ter implicações de custo fora do plano gratuito. Um artigo detalhou a integração de LangFlow com AstraDB . * **Recuperação (Retrieval)**: Quando o usuário fizer uma pergunta, ela será convertida em um embedding (usando o mesmo modelo de embedding). Esse embedding de consulta será usado para buscar os "k" chunks mais similares no banco de dados vetorial (FAISS ou Astra DB). * **Geração de Resposta (LangFlow + Kimie K2 via OpenRouter)**: * **Integração com LLM**: O LangFlow será configurado para enviar o prompt, enriquecido com os chunks relevantes recuperados e o contexto da persona, para um modelo de linguagem grande (LLM). * **Kimie K2 Free via OpenRouter**: O LLM escolhido será o Kimie K2 Free, acessado através da API da OpenRouter. A OpenRouter atua como uma camada de abstração, simplificando o acesso a diversos modelos LLM. A documentação do Kimi K2 , e seu repositório GitHub , indicam que é um modelo MoE (Mixture-of-Experts) com 32 bilhões de parâmetros ativados e 1 trilhão de parâmetros no total, projetado para tarefas de código e agentes. * **Prompt Engineering**: Será utilizado um `PromptTemplate` no LangFlow ou no backend Python para estruturar o input para o LLM. Este template será dinamicamente preenchido com o contexto da persona selecionada (técnica ou leiga), a pergunta do usuário e os chunks de texto relevantes recuperados do banco vetorial. * **Publicação como API**: O fluxo construído no LangFlow será publicado como uma API, permitindo que a aplicação web Python (backend) faça chamadas para obter as respostas do chatbot. 3. **Backend Python (Intermediador)**: * **Comunicação com o Chatbot**: O backend Python, parte da aplicação web no Render, será responsável por receber as requisições do frontend (interação do usuário com o chatbot) e a seleção da persona. * **Injeção Dinâmica de Persona**: Com base na escolha do usuário, o backend injetará dinamicamente as instruções da persona (Dr. Gasnelio - técnica, ou Gá - leiga) no `PromptTemplate` antes de enviar a solicitação para a API do LangFlow ou diretamente para o LLM via OpenRouter. * **Gerenciamento de Conversa**: O backend também poderá ser responsável por gerenciar o estado da conversa, como o histórico de mensagens, se necessário para a coesão das respostas, embora o LangFlow também possa lidar com aspectos de memória de conversação. **Fluxo de Dados:** 1. **Inicialização/Atualização da Base de Conhecimento**: * O conteúdo da tese de doutorado é carregado e processado (splitter). * Os chunks de texto são convertidos em embeddings. * Os embeddings são armazenados no banco de dados vetorial (FAISS ou Astra DB). * Este processo pode ser executado como um script separado ou como parte da configuração inicial do serviço LangFlow. 2. **Interação do Usuário com o Chatbot**: * O usuário insere uma pergunta e seleciona uma persona no frontend (Streamlit). * A aplicação web (backend Python no Render) recebe a pergunta e a persona. * O backend formata a requisição, incluindo a pergunta e o contexto da persona. * A requisição é enviada para a API do fluxo LangFlow. * O LangFlow converte a pergunta em um embedding. * O embedding da pergunta é usado para buscar chunks relevantes no banco de dados vetorial. * Os chunks recuperados são combinados com o prompt (incluindo a persona, injetada pelo backend ou LangFlow) e enviados para o Kimie K2 via OpenRouter. * O Kimie K2 gera a resposta. * A resposta é retornada pela API do LangFlow para o backend Python (ou diretamente pela API da OpenRouter). * O backend Python encaminha a resposta para o frontend para exibição ao usuário. **Integrações de API:** * **OpenRouter API**: Utilizada pelo backend Python (ou pelo LangFlow, dependendo da configuração) para acessar o modelo de linguagem Kimie K2 Free . * **API do Fluxo LangFlow**: Exposta pelo LangFlow para receber as consultas do backend Python e retornar as respostas do chatbot (após a etapa RAG). * **API do Banco de Dados Vetorial (se aplicável)**: Se Astra DB for usado, haverá uma integração via sua API. Se FAISS for usado localmente, a integração será através da biblioteca FAISS em Python. **Controle de Versão:** * **Repositório Git**: Um novo repositório, `roteiro-de-dispersacao-v4`, será criado para o projeto. Apenas os arquivos essenciais para o funcionamento do site e do chatbot serão clonados do repositório original (se e quando o acesso for estabelecido) ou desenvolvidos do zero. * **Branches e Pull Requests**: Será adotado um fluxo de trabalho baseado em branches, com branches de funcionalidade (`feature branches`) sendo mescladas na branch principal (`main` ou `master`) através de Pull Requests, após revisão de código. * **Render.yaml e CI/CD**: O arquivo `render.yaml` no repositório controlará a configuração de implantação no Render, e as atualizações no repositório (git push) acionarão automaticamente o pipeline de CI/CD. **Diagrama de Arquitetura Conceitual:** ```mermaid graph TD A[Usuário] -->|Pergunta, Persona| B(Frontend - Streamlit); B -->|Requisição HTTP| C(Backend Python - App Render (Flask/FastAPI + Gunicorn)); C -->|Chama API com Prompt| D(LangFlow - Fluxo RAG); D -->|Carrega/Atualiza Dados| E(Base de Conhecimento - Tese (PDF/Markdown)); E -->|Texto| F(Loader - MarkdownLoader/PyPDFLoader); F -->|Chunks de Texto| G(Text Splitter); G -->|Chunks| H(Embedding Model - Hugging Face); H -->|Vetores| I(Vector Store - FAISS/Astra DB); D -->|Consulta Embedding| I; I -->|Chunks Relevantes| D; D -->|Retorna Contexto RAG| C; C -->|Prompt + Chunks + Persona| J(LLM - Kimie K2 Free via OpenRouter API); J -->|Resposta| C; C -->|Resposta| B; B -->|Exibe Resposta| A; subgraph Plataforma Render C end subgraph Infraestrutura Chatbot (LangFlow) D E F G H I end subgraph Serviço de LLM (OpenRouter) J end %% Legendas (não fazem parte do diagrama funcional) classDef user fill:#D5F5E3,stroke:#333,stroke-width:2px; classDef render fill:#FEF9E7,stroke:#333,stroke-width:2px; classDef chatbot fill:#FADBD8,stroke:#333,stroke-width:2px; classDef llm fill:#E8F8F5,stroke:#333,stroke-width:2px; class A user; class B,C render; class D,E,F,G,H,I chatbot; class J llm; ``` Esta arquitetura visa atender aos requisitos de modernização, suporte a duas personas, utilização de RAG com LangFlow e Kimie K2, e implantação contínua no Render, mantendo o foco na gratuidade das ferramentas e serviços utilizados. ### 3.8. Tecnologias e Ferramentas A modernização do site e do chatbot de Roteiro de Dispensação será baseada em um conjunto de tecnologias e ferramentas modernas, escolhidas para atender aos requisitos de funcionalidade, desempenho, custo zero (gratuidade) e facilidade de desenvolvimento e implantação. A seleção prioriza soluções de código aberto ou com planos gratuitos robustos, alinhadas com as diretrizes do projeto. **Principais Tecnologias e Ferramentas:** 1. **Linguagem de Programação e Framework Web:** * **Python**: A linguagem principal para o desenvolvimento do backend da aplicação web e para a lógica de integração do chatbot. Python é amplamente utilizado em projetos de IA e machine learning, possui uma vasta coleção de bibliotecas e é suportado pela plataforma Render. * **Gunicorn**: Um servidor WSGI HTTP para Unix, utilizado para servir a aplicação Python no Render. Ele é robusto e eficiente para aplicações web Python em produção. * **Flask/FastAPI**: Framework web Python para a criação da API backend. A escolha específica dependerá da complexidade e preferências da equipe. * **Streamlit**: Biblioteca Python para criação de interfaces web interativas, utilizada para o frontend do chatbot . 2. **Plataforma de Hospedagem e CI/CD:** * **Render**: A plataforma de nuvem escolhida para hospedar a aplicação web Python. Render oferece um plano gratuito que inclui implantação contínua a partir de um repositório Git, gerenciamento de variáveis de ambiente e Secret Files para segurança. O arquivo `render.yaml` será utilizado para configurar os serviços e o pipeline de CI/CD, com gatilhos por `git push`. 3. **Chatbot - Arquitetura RAG e Modelo de Linguagem:** * **RAG (Retrieval-Augmented Generation)**: A arquitetura escolhida para o chatbot, permitindo que ele responda com base em uma base de conhecimento específica (a tese de doutorado) e evite alucinações comuns em LLMs. * **LangFlow**: Uma ferramenta de interface gráfica para construir e executar fluxos de LangChain. Será utilizada para implementar o pipeline RAG, incluindo carregamento de documentos, splitting, embedding, armazenamento vetorial e integração com o LLM. A publicação do fluxo como uma API permitirá a comunicação com o backend Python. * **LangChain**: Biblioteca para desenvolvimento de aplicações com modelos de linguagem, integrando diversos componentes do pipeline RAG. * **Hugging Face Transformers/Sentence-Transformers**: Bibliotecas para obter modelos de embedding de texto. Modelos como `sentence-transformers/all-MiniLM-L6-v2` são comumente usados para converter texto em vetores semânticos. * **FAISS (Facebook AI Similarity Search)**: Uma biblioteca para busca eficiente de similaridade em vetores e agrupamento. Será a opção primária para o banco de dados vetorial, especialmente considerando a premissa de gratuidade e sua capacidade de ser executada localmente. Vários exemplos mostram sua integração em pipelines RAG com Python , . * **Astra DB (Alternativa)**: Um banco de dados vetorial gerenciado baseado em Apache Cassandra, oferecido pela DataStax. Pode ser considerado como uma alternativa ao FAISS se houver necessidade de um serviço gerenciado e escalável, sujeito à disponibilidade de um plano gratuito adequado. A integração de LangFlow com Astra DB foi demonstrada em um artigo . * **Kimie K2 Free**: O modelo de linguagem grande (LLM) escolhido para gerar as respostas do chatbot. É um modelo MoE (Mixture-of-Experts) desenvolvido pela Moonshot AI, com 32B parâmetros ativados e 1T parâmetros no total, conhecido por suas capacidades em código e tarefas de agente , . * **OpenRouter**: Uma plataforma que fornece uma API unificada para acessar diversos LLMs, incluindo o Kimie K2 Free. Será utilizada para integrar o Kimie K2 ao fluxo LangFlow ou ao backend Python, simplificando o acesso e o gerenciamento de chaves de API . * **PromptTemplates**: Estruturas de prompt dinâmicas injetadas via backend para controlar o comportamento (persona) do LLM , . 4. **Controle de Versão e Colaboração:** * **Git**: O sistema de controle de versão padrão para gerenciar o código-fonte do projeto. * **GitHub**: A plataforma de hospedagem de repositórios Git. Um novo repositório, `roteiro-de-dispersacao-v4`, será criado para este projeto. A integração com o Render permitirá a CI/CD automática. 5. **Processamento de Documentos (Base de Conhecimento):** * **MarkdownLoader / PyPDFLoader**: Componentes do LangChain (e, portanto, do LangFlow) para carregar o conteúdo da tese de doutorado, que pode estar em formatos como Markdown ou PDF. * **Text Splitters (ex: RecursiveCharacterTextSplitter)**: Para dividir o documento carregado em pedaços menores (chunks) adequados para a criação de embeddings e recuperação eficiente. **Tabela de Tecnologias e Suas Funções:** | Tecnologia/Ferramenta | Categoria | Função no Projeto | Licença/Custo (Premissa) | |----------------------------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------| | Python | Linguagem de Programação | Backend da aplicação web, lógica de integração do chatbot. | Open Source / Gratuito | | Gunicorn | Servidor WSGI | Servir a aplicação Python no ambiente de produção (Render). | Open Source / Gratuito | | Flask/FastAPI | Framework Web Python | Criação da API backend da aplicação web. | Open Source / Gratuito | | Streamlit | Biblioteca Frontend Python | Criação da interface do usuário (UI) do chatbot. Exemplo: `manojknit/PDF-AnswerHub-GenAI-ChatBot` . | Open Source / Gratuito | | Render | Plataforma de Nuvem | Hospedagem da aplicação web, CI/CD, gerenciamento de variáveis e secrets. | Plano Gratuito | | LangFlow | Framework RAG (UI) | Construção e execução do pipeline RAG, publicação como API. | Open Source / Gratuito | | LangChain | Biblioteca LLM | Desenvolvimento de aplicações com modelos de linguagem, integrando componentes do pipeline RAG. | Open Source / Gratuito | | Hugging Face Embeddings | Modelos de Embedding | Conversão de texto (chunks e perguntas) em vetores semânticos. Exemplo: `sentence-transformers/all-MiniLM-L6-v2` . | Open Source / Gratuito | | FAISS | Banco de Dados Vetorial | Armazenamento e busca de similaridade dos embeddings da base de conhecimento (opção local). | Open Source / Gratuito | | Astra DB (Alternativa) | Banco de Dados Vetorial | Armazenamento e busca de similaridade dos embeddings (opção gerenciada em nuvem) . | Plano Gratuito (?) | | Kimie K2 Free | Modelo de Linguagem (LLM) | Geração das respostas do chatbot com base no prompt e contexto RAG. | Gratuito via OpenRouter | | OpenRouter | Plataforma de LLMs | Fornecer acesso à API do Kimie K2 Free e outros LLMs . | Plano Gratuito | | PromptTemplates | Engenharia de Prompt | Injeção dinâmica de contexto de persona (técnica ou leiga) para customizar as respostas do LLM , . | - | | Git | Controle de Versão | Gerenciamento do código-fonte do projeto. | Open Source / Gratuito | | GitHub | Hospedagem de Código | Repositório do projeto (`roteiro-de-dispersacao-v4`), integração com CI/CD do Render. | Gratuito | | MarkdownLoader/PyPDFLoader | Processamento de Texto | Carregar o conteúdo da tese de doutorado para o pipeline RAG. | Open Source (via LangChain) | | Text Splitters | Processamento de Texto | Dividir o documento carregado em chunks para embedding. | Open Source (via LangChain) | A combinação dessas tecnologias permitirá a construção de um sistema moderno, eficiente e alinhado com os objetivos de modernização e gratuidade do projeto. A escolha de ferramentas como LangFlow e Render visa simplificar o desenvolvimento e a implantação, enquanto o uso de FAISS e Kimie K2 Free via OpenRouter atende aos requisitos de custo e funcionalidade do chatbot. ## 4. Revisão (R) ### 4.1. Análise do Site e Repositório Atual A análise inicial do site "Roteiro de Dispensação para Hanseníase", hospedado na plataforma Render (<https://repositorio-roteiro-de-dispersacao.onrender.com/>), permitiu identificar sua estrutura básica e funcionalidades . O site apresenta um título claro, "Tese de Doutorado - Roteiro de Dispensação para Hanseníase", e uma navegação composta por links como "Início", "Sobre", "A Tese", "Chatbot" e "Contato". Além disso, foram observados botões de ação, como "Leia a Tese Completa" e "Conversar com a IA", indicando a integração de um chatbot. No entanto, a tentativa de acessar o repositório GitHub associado, supostamente localizado em <https://github.com/AnalineS/repositorio_roteiro_de_dispersacao>, não foi bem-sucedida, pois a busca não retornou resultados . Isso sugere que o repositório pode ser privado, o URL fornecido está incorreto ou o repositório foi removido. A interação com o elemento "Chatbot" ou o botão "Conversar com a IA" também enfrentou obstáculos devido a erros técnicos nas ferramentas de análise utilizadas, impedindo uma avaliação mais profunda da implementação atual do chatbot através da inspeção do frontend , . Portanto, a revisão do código-fonte existente e da infraestrutura de implantação atual depende da obtenção de acesso ao repositório correto ou de uma análise mais aprofundada do site em produção, se possível. A estratégia de criar um novo repositório (`roteiro-de-dispersacao-v4`) e clonar apenas os arquivos essenciais se torna mais relevante diante dessa limitação. ### 4.2. Análise do Chatbot Atual A análise do chatbot atualmente integrado ao site "Roteiro de Dispensação para Hanseníase" revelou a existência de pelo menos uma persona, denominada **"Dr. Gasnelio"**, que opera no modo **"Professor"** . Este modo indica uma abordagem técnica e especializada, alinhada com o propósito de disseminar informações contidas na tese de doutorado que serve como base de conhecimento. A interação inicial com o Dr. Gasnelio apresenta uma saudação formal e informativa: "Saudações! Sou o Dr. Gasnelio. Minha pesquisa foca no roteiro de dispensação para a prática da farmácia clínica. Como posso auxiliá-lo hoje?" . Esta fala introdutória estabelece claramente a identidade da persona e sua área de expertise, direcionando o usuário para um tipo de interação baseada em consulta técnica. A interface do chatbot, acessada através do link "Conversar com a IA" , sugere a possibilidade de escolha de persona, conforme indicado pela mensagem "Escolha uma persona e interaja!" . No entanto, durante a análise, apenas a persona "Professor" foi explicitamente identificada e testada. A ausência de menção ou acesso fácil à segunda persona, **"Amigo" (ou "Gá")**, sugere que esta possa não estar plenamente implementada, ou sua presença na interface não é tão evidente quanto a do Dr. Gasnelio. A tentativa de localizar termos como "Amigo" ou "Gá" nas páginas analisadas do site não obteve sucesso, reforçando a hipótese de que a funcionalidade de dualidade de personas possa estar incompleta ou requerer um fluxo de interação específico ainda não identificado. A investigação sobre a infraestrutura e a tecnologia por trás do chatbot atual indicou a utilização de uma conexão **WebSocket**, especificamente `wss://ws.render.com/` . Esta observação sugere que o componente de chat pode ser um serviço independente, possivelmente também hospedado na plataforma Render, que se comunica com o front-end do site através deste protocolo. No entanto, a análise não conseguiu determinar com clareza se este WebSocket é um serviço genérico da Render ou uma instância específica para o projeto. Apesar das tentativas de encontrar mais detalhes sobre a implementação do chatbot, como bibliotecas JavaScript específicas ou integrações de API no código-fonte visível, essas informações não foram localizadas nas páginas analisadas , . Isso pode indicar que o chatbot é construído com uma ferramenta ou framework que não deixa rastros explícitos no HTML ou que sua lógica principal reside em um backend não diretamente inspecionável através da interface do usuário. A ausência de detalhes sobre como a persona "Professor" é estruturada em termos de prompt ou como a base de conhecimento (a tese) é integrada ao chatbot atual também foi uma limitação desta fase de revisão. Portanto, a modernização proposta precisará construir uma nova solução de chatbot, considerando as lacunas de informação sobre a implementação existente e focando nos requisitos de dualidade de personas e integração RAG com a tese. A análise da interação com o Dr. Gasnelio, na persona "Professor", demonstrou um alinhamento com o perfil de um especialista técnico. A linguagem utilizada é culta e profissional, refletindo o tom esperado de um acadêmico ou profissional de saúde experiente. Por exemplo, a saudação inicial "Saudações! Sou o Dr. Gasnelio. Minha pesquisa foca no roteiro de dispensação para a prática da farmácia clínica. Como posso auxiliá-lo hoje?" é direta, informativa e estabelece um contexto claro para a conversa. Esta abordagem é adequada para stakeholders como estudantes de doutorado, professores e pesquisadores que buscam informações detalhadas e precisas sobre a tese e o roteiro de dispensação. No entanto, a ausência da persona "Amigo" impede uma avaliação completa da capacidade atual do sistema em atender a um público leigo, que requer uma linguagem mais simples, empática e acessível. A expectativa para a persona "Amigo" (Gá) é que ela utilize uma linguagem cotidiana, seja bem-humorada, empática e capaz de explicar conceitos complexos de forma simplificada, até mesmo para crianças ou pessoas sem formação acadêmica. Como essa persona não foi encontrada durante a revisão, considera-se que esta é uma área crítica para a modernização, visando ampliar a acessibilidade e a usabilidade do chatbot para um público mais diversificado, incluindo pacientes e seus familiares. A nova implementação deverá, portanto, garantir que ambas as personas sejam claramente disponibilizadas e que suas respectivas características de interação sejam consistentemente aplicadas. ### 4.3. Revisão da Base de Conhecimento (Tese) A **base de conhecimento central para o chatbot modernizado será a tese de doutorado intitulada "Roteiro de Dispensação para Hanseníase"**. A análise desta tese é crucial para o sucesso da implementação do sistema RAG (Retrieval-Augmented Generation). A revisão da tese envolverá a compreensão de sua estrutura, formato (presumivelmente PDF ou Markdown), volume de texto, complexidade da linguagem e a natureza das informações contidas. É necessário identificar se a tese contém diagramas, tabelas ou referências extensas que possam requerer processamento especial durante a ingestão de dados. A qualidade e a clareza do texto na tese impactarão diretamente a eficácia da geração de embeddings e a relevância dos trechos recuperados pelo sistema RAG. A revisão também deve considerar a possibilidade de extensão do escopo para outras doenças no futuro, conforme mencionado nas diretrizes do projeto. Se a tese atual for muito específica para hanseníase, a arquitetura de base de conhecimento e o pipeline RAG devem ser projetados de forma modular para acomodar futuras inclusões de dados sobre outras patologias sem grandes refatorações. A consistência terminológica e a organização do conteúdo na tese também são fatores importantes a serem avaliados, pois influenciarão a precisão das respostas geradas pelo chatbot. Esta etapa de revisão da base de conhecimento é fundamental para definir os parâmetros de carregamento, divisão de texto (chunking) e geração de embeddings, garantindo que o sistema RAG possa recuperar as informações mais relevantes para as consultas dos usuários. ### 4.4. Avaliação da Plataforma Render A plataforma **Render** foi escolhida para hospedar o site e os serviços backend do projeto. Uma avaliação preliminar indica que o Render oferece um **plano gratuito** que pode atender às necessidades iniciais do projeto, incluindo suporte a aplicações web Python, implantação contínua (CI/CD) via integração com Git e arquivos `render.yaml`, e gerenciamento de variáveis de ambiente e Secret Files para informações sensíveis. No entanto, é importante considerar as **limitações inerentes ao plano gratuito**, como restrições de recursos de CPU e RAM, tempo de atividade (possibilidade de serviços entrarem em sono após inatividade, causando atraso no primeiro carregamento) e largura de banda . A escalabilidade também é limitada neste plano. A documentação do Render para implantação de aplicativos foi apontada como uma área que poderia ser melhorada em algumas avaliações , o que pode representar um desafio durante a configuração. A plataforma oferece SSL automático, o que é benéfico para segurança. A criação de **serviços Blueprint** no Render pode ser explorada para gerenciar múltiplos serviços de forma coordenada, se aplicável. A busca por diagramas de arquitetura específicos do Render não foi conclusiva, mas conceitos gerais de arquitetura de nuvem e CI/CD são aplicáveis , . A decisão de utilizar o Render está alinhada com a premissa de custo zero do projeto, mas a equipe de desenvolvimento deve estar ciente de suas limitações e planejar a arquitetura e a otimização de recursos de acordo. O sucesso da hospedagem no Render dependerá da eficiência da aplicação e da configuração adequada do ambiente e do pipeline de CI/CD. ## 5. Execução (E) ### 5.1. Configuração do Ambiente e Infraestrutura A configuração do ambiente e da infraestrutura será realizada na plataforma **Render**. Inicialmente, um novo repositório Git, denominado `roteiro-de-dispersacao-v4`, será criado no GitHub para abrigar todo o código-fonte do projeto. A partir deste repositório, a implantação contínua (CI/CD) será configurada no Render utilizando um arquivo `render.yaml`. Este arquivo definirá os serviços necessários, como a aplicação web Python, e especificará os comandos de build e start, além de configurar o gunicorn como servidor WSGI. As variáveis de ambiente necessárias para o funcionamento da aplicação, como chaves de API para o OpenRouter e configurações de banco de dados vetorial (se aplicável), serão gerenciadas de forma segura através dos **Secret Files** e da estrutura de variáveis de ambiente do Render, evitando o hardcoding de informações sensíveis no código. A infraestrutura será projetada para ser o mais simples possível, considerando a premissa de gratuidade, mas com a possibilidade de utilizar **Blueprints do Render** se houver a necessidade de gerenciar múltiplos serviços inter-relacionados (por exemplo, um serviço para a aplicação web e outro para um worker de processamento de dados, se necessário). O ambiente de desenvolvimento local dos desenvolvedores deve ser configurado para refletir o ambiente de produção no Render, utilizando virtualenv para isolamento de dependências e um arquivo `requirements.txt` ou `pyproject.toml` para gerenciamento de pacotes Python. ### 5.2. Desenvolvimento e Integração do Chatbot O desenvolvimento do chatbot envolverá a implementação da arquitetura **RAG (Retrieval-Augmented Generation)** utilizando a ferramenta **LangFlow**. O fluxo no LangFlow será construído para processar a base de conhecimento (tese de doutorado), que será carregada usando componentes como `MarkdownLoader` ou `PyPDFLoader`. O texto será então dividido em "chunks" menores e gerenciáveis com um `TextSplitter` (ex: `RecursiveCharacterTextSplitter`). Em seguida, os chunks serão convertidos em embeddings vetoriais utilizando modelos do **Hugging Face** (ex: `sentence-transformers/all-MiniLM-L6-v2` ). Esses embeddings serão armazenados e indexados em um banco de dados vetorial, com **FAISS** sendo a opção primária devido à sua natureza local e gratuita, ou **Astra DB** como alternativa gerenciada, se viável dentro do orçamento zero. Quando o usuário fizer uma pergunta, ela será convertida em um embedding e o banco de dados vetorial será consultado para recuperar os chunks mais relevantes da tese. O fluxo LangFlow será então configurado para publicar uma API que recebe a pergunta do usuário e retorna os trechos de contexto relevantes. O backend Python da aplicação web se comunicará com esta API LangFlow e, posteriormente, com a API do OpenRouter para gerar a resposta final usando o Kimie K2, conforme detalhado na seção 5.5. A interface do usuário do chatbot será desenvolvida com **Streamlit**, permitindo a seleção da persona e a interação em formato de chat . ### 5.3. Desenvolvimento e Atualização do Site O desenvolvimento e a atualização do site focarão na criação de uma **aplicação web Python hospedada no Render**, utilizando um framework como Flask ou FastAPI para o backend e Streamlit para o frontend do chatbot. O site deve apresentar uma navegação clara, incluindo seções como "Início", "Sobre o Projeto", "A Tese", "Chatbot" e "Contato". O conteúdo estático será atualizado conforme necessário para refletir as novas funcionalidades e a modernização. A integração do chatbot desenvolvido com Streamlit será feita de forma fluida, garantindo uma experiência de usuário coesa. O site será projetado para ser responsivo, adaptando-se a diferentes dispositivos e tamanhos de tela. O backend Python será responsável por servir as páginas estáticas, gerenciar as requisições do frontend do chatbot, interagir com o LangFlow (para o RAG) e com a OpenRouter (para o LLM Kimie K2). A lógica de injeção dinâmica da persona selecionada pelo usuário ("Dr. Gasnelio" ou "Gá") no `PromptTemplate` também será implementada no backend Python. O objetivo é criar um site moderno, de fácil utilização e que sirva como uma plataforma eficaz para a disseminação do conhecimento contido na tese de doutorado. ### 5.4. Implementação da Pipeline CI/CD A **pipeline de Integração Contínua/Entrega Contínua (CI/CD)** será implementada utilizando os recursos nativos da plataforma **Render**, acionados por alterações no repositório Git (`roteiro-de-dispersacao-v4`). A configuração da pipeline será definida através de um arquivo `render.yaml` (ou `render.yml`) na raiz do repositório. Este arquivo especificará os serviços a serem implantados (por exemplo, a aplicação web Python), os comandos para construir a aplicação (ex: `pip install -r requirements.txt`), os comandos para iniciar o serviço (ex: `gunicorn app:app`), e as configurações de ambiente. Sempre que um desenvolvedor fizer um `git push` para a branch principal (ex: `main` ou `master`) do repositório, o Render automaticamente iniciará o processo de CI/CD. Isso incluirá a clonagem do código mais recente, a execução dos comandos de build e, se bem-sucedido, a implantação da nova versão da aplicação. O uso da CI/CD automatizada garante que as atualizações sejam implantadas de forma rápida e consistente, reduzindo erros manuais e agilizando o ciclo de desenvolvimento. A configuração também deve incluir o gerenciamento de variáveis de ambiente e Secret Files diretamente no painel do Render, para que sejam injetados corretamente durante o processo de build e execução. ### 5.5. Integração com OpenRouter e Kimie K2 A integração da nova camada de backend com o modelo de linguagem **Kimie K2 Free**, acessado gratuitamente por meio da plataforma **OpenRouter**, é um componente crítico para a evolução do chatbot. Esta integração permitirá que o chatbot gere respostas mais modernas, contextualizadas e alinhadas com as personas definidas (técnica e leiga). O processo de integração envolverá a utilização da API da OpenRouter, que é compatível com a API da OpenAI, facilitando a adaptação de código e bibliotecas existentes. Conforme demonstrado em exemplos de código , a comunicação com a OpenRouter pode ser estabelecida utilizando a biblioteca `openai` em Python, configurando-a para apontar para o endpoint da OpenRouter e fornecendo a chave de API específica da OpenRouter. A chave de API da OpenRouter será armazenada de forma segura utilizando **Secret Files no Render**, garantindo que não seja exposta no código-fonte ou repositório. O modelo Kimie K2 será especificado no campo `model` da solicitação de conclusão de chat, por exemplo, `model="moonshotai/kimi-k2"` ou uma variante mais específica como `model="moonshotai/kimi-k2-13b-v1.0"` . A mensagem enviada para a API conterá o prompt construído, que incluirá o contexto da persona selecionada (Dr. Gasnelio ou Gá), os trechos relevantes da tese recuperados pela etapa RAG e a pergunta original do usuário. O Kimie K2, com sua arquitetura de 1 trilhão de parâmetros e janela de contexto de 128K tokens , é adequado para processar informações complexas e gerar respostas coerentes e detalhadas. A escolha da OpenRouter como intermediária proporciona acesso a esse modelo poderoso de forma gratuita, dentro dos limites de uso estabelecidos pela plataforma, o que é crucial para o orçamento do projeto. A implementação dessa integração ocorrerá na camada de backend da aplicação Python, que receberá as solicitações do chatbot montado no LangFlow, preparará o payload para a OpenRouter, fará a chamada à API e processará a resposta retornada pelo Kimie K2 antes de enviá-la de volta para a interface do usuário. A robustez dessa integração será testada, considerando possíveis falhas na rede ou na API, e implementando mecanismos de retentativa ou fallback, se viável dentro da premissa de gratuidade. A documentação da OpenRouter e do Kimie K2 fornecerá detalhes adicionais sobre parâmetros da API, formatos de entrada e saída, e melhores práticas para otimizar a interação com o modelo . O fluxo de integração detalhado pode ser descrito da seguinte forma: 1. **Configuração da Chave de API**: A chave de API da OpenRouter será obtida através do painel da OpenRouter e configurada como uma variável de ambiente segura (Secret File) no serviço Render onde a aplicação Python backend está hospedada. Isso evita o hardcoding da chave no código-fonte. 2. **Instalação da Biblioteca Cliente**: A biblioteca `openai` do Python será utilizada para interagir com a API da OpenRouter. Se necessário, outras bibliotecas específicas da OpenRouter ou compatíveis podem ser consideradas. A instalação será gerenciada via `requirements.txt` ou `pyproject.toml`. 3. **Inicialização do Cliente OpenAI Modificado**: No código Python do backend, o cliente da OpenAI será configurado para apontar para o endpoint da OpenRouter e utilizar a chave de API fornecida. Um exemplo de código seria: ```python from openai import OpenAI client = OpenAI( api_key="SUA_CHAVE_OPENROUTER_AQUI", # Na prática, isso virá de uma variável de ambiente base_url="https://openrouter.ai/api/v1" ) ``` É importante notar que a OpenRouter também suporta cabeçalhos HTTP adicionais como `HTTP-Referer` e `X-Title` para rastreamento e estatísticas, que podem ser opcionalmente incluídos . 4. **Construção da Mensagem (Prompt)**: O backend receberá do LangFlow ou do frontend a pergunta do usuário, a persona selecionada e os trechos de contexto recuperados pelo RAG. Um `PromptTemplate` será usado para estruturar essas informações em uma ou mais mensagens no formato esperado pela API (geralmente uma lista de dicionários com `role` - "system", "user", "assistant" - e `content`). Por exemplo: ```python # Exemplo simplificado de construção de prompt system_message = {"role": "system", "content": "Você é um assistente especializado em teses de doutorado sobre roteiros de dispensação de medicamentos."} if persona == "Dr. Gasnelio": system_message["content"] += " Responda de forma técnica, culta e profissional, como um professor de mestrado." elif persona == "Gá": system_message["content"] += " Responda de forma simples, amigável, bem-humorada e empática, como um amigo explicando para uma criança." user_message = {"role": "user", "content": f"Contexto: {contexto_rag}\n\nPergunta: {pergunta_usuario}"} messages = [system_message, user_message] ``` 5. **Chamada à API da OpenRouter**: Com as mensagens construídas e o modelo Kimie K2 especificado, a chamada à API será realizada. Exemplo: ```python try: response = client.chat.completions.create( model="moonshotai/kimi-k2", # Pode ser um modelo específico como "moonshotai/kimi-k2-13b-v1.0" messages=messages, # Outros parâmetros opcionais como temperature, max_tokens podem ser adicionados ) resposta_bruta = response.choices[0].message.content except Exception as e: # Tratamento de erros (ex.: falha de rede, limite de taxa da API) print(f"Erro ao chamar a API da OpenRouter: {e}") resposta_bruta = "Desculpe, ocorreu um erro ao processar sua solicitação." ``` 6. **Processamento da Resposta**: A resposta bruta do Kimie K2 pode requerer pós-processamento, como limpeza de formatação, extração de informações específicas ou tratamento de casos em que o modelo não fornece uma resposta útil. 7. **Retorno da Resposta**: A resposta final processada é então retornada pelo backend para o LangFlow ou diretamente para o frontend, para ser exibida ao usuário. A documentação da OpenRouter e do Kimie K2 fornece detalhes adicionais sobre os modelos disponíveis, parâmetros da API (como `temperature`, `max_tokens`, `stream`), limites de taxa e práticas recomendadas. A Moonshot AI também disponibiliza a API do Kimie K2 diretamente em `https://platform.moonshot.ai` com compatibilidade OpenAI/Anthropic , mas para este projeto, a via OpenRouter é preferida devido à premissa de gratuidade. A gestão da chave de API é crucial, e a OpenRouter oferece um painel para monitorar o uso e gerenciar chaves . A integração bem-sucedida permitirá que o chatbot aproveite as capacidades avançadas do Kimie K2, como seu extenso contexto e habilidades de raciocínio, para fornecer um serviço de alta qualidade aos usuários. ## 6. Validação (V) ### 6.1. Critérios de Aceitação para o Site Os critérios de aceitação para o site modernizado são essenciais para garantir que ele atenda aos requisitos de funcionalidade, usabilidade e desempenho. Estes critérios serão utilizados durante a fase de Validação para verificar se o site está pronto para a implantação em produção. **Critérios de Aceitação para o Site (CA-S):** | ID | Descrição do Critério | Método de Verificação | Prioridade | |--------|----------------------------------------------------------------------------------------------------------------------|----------------------------|------------| | CA-S01 | O site deve ser acessível publicamente através da URL fornecida pelo Render, sem erros de carregamento. | Teste manual de acesso | Alta | | CA-S02 | Todas as páginas estáticas (Início, Sobre, A Tese, Contato) devem ser carregadas corretamente e exibir o conteúdo esperado. | Teste manual de navegação | Alta | | CA-S03 | A navegação entre as diferentes seções do site deve ser intuitiva e funcionar sem erros. | Teste manual de navegação | Alta | | CA-S04 | O site deve ser responsivo, adaptando seu layout corretamente para diferentes tamanhos de tela (desktop, tablet, mobile). | Teste manual em múltiplos dispositivos/resoluções | Alta | | CA-S05 | O link "Leia a Tese Completa" deve redirecionar para o documento da tese de forma correta. | Teste manual do link | Média | | CA-S06 | A interface do chatbot (desenvolvida com Streamlit) deve ser carregada e integrada ao site de forma fluida. | Teste manual de interação | Alta | | CA-S07 | O processo de CI/CD deve ser acionado com um `git push` para a branch principal e implantar a nova versão do site no Render sem intervenção manual (além da aprovação do PR, se aplicável). | Observação do processo de CI/CD | Alta | | CA-S08 | As variáveis de ambiente e Secret Files devem ser configuradas corretamente no Render, permitindo o funcionamento da aplicação sem vazamento de informações sensíveis. | Verificação de configuração no Render e teste de funcionalidade | Crítica | | CA-S09 | O site deve carregar em um tempo aceitável, considerando as limitações do plano gratuito do Render. | Teste de desempenho (latência de carregamento) | Média | A validação bem-sucedida destes critérios indicará que o site está tecnicamente sólido, funcional e pronto para uso pelos stakeholders e pelo público-alvo. ### 6.2. Critérios de Aceitação para o Chatbot Os critérios de aceitação para o chatbot modernizado são fundamentais para assegurar que ele atenda aos requisitos de funcionalidade, precisão e adaptabilidade das personas. Estes critérios guiarão os testes durante a fase de Validação. **Critérios de Aceitação para o Chatbot (CA-C):** | ID | Descrição do Critério | Método de Verificação | Prioridade | |--------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|------------| | CA-C01 | O usuário deve conseguir selecionar entre as personas "Dr. Gasnelio" (técnica) e "Gá" (leiga) através da interface do chatbot. | Teste manual de seleção de persona | Alta | | CA-C02 | O chatbot deve gerar respostas para perguntas relacionadas à tese de doutorado sobre o roteiro de dispensação para hanseníase. | Teste manual com perguntas variadas | Alta | | CA-C03 | As respostas do chatbot devem ser baseadas no conteúdo da tese de doutorado (RAG), evitando alucinações ou informações não contidas na base de conhecimento. | Teste manual com perguntas dentro e fora do escopo | Crítica | | CA-C04 | A persona "Dr. Gasnelio" deve responder com linguagem técnica, culta e profissional, adequada para um público especializado. | Teste manual de usabilidade e análise de respostas | Alta | | CA-C05 | A persona "Gá" deve responder com linguagem simples, cotidiana, empática e bem-humorada, adequada para um público leigo, incluindo crianças. | Teste manual de usabilidade e análise de respostas | Alta | | CA-C06 | O tempo de resposta do chatbot (latência) deve ser aceitável, idealmente abaixo de 10 segundos, considerando as limitações das ferramentas gratuitas. | Teste de desempenho (latência de resposta) | Média | | CA-C07 | O fluxo RAG implementado no LangFlow deve publicar uma API funcional que retorna trechos de contexto relevantes para as perguntas. | Teste de integração da API LangFlow | Alta | | CA-C08 | A integração com a API da OpenRouter para acessar o Kimie K2 Free deve funcionar corretamente, gerando respostas a partir do LLM. | Teste de integração da API OpenRouter | Crítica | | CA-C09 | O `PromptTemplate` deve injetar dinamicamente o contexto da persona selecionada e os trechos RAG no prompt enviado ao LLM. | Análise de logs/rastreamento de prompt | Alta | | CA-C10 | O chatbot deve lidar adequadamente com perguntas fora do escopo ou que não podem ser respondidas com a base de conhecimento atual (ex: "Não sei responder isso"). | Teste manual com perguntas fora do escopo | Média | A validação destes critérios garantirá que o chatbot esteja funcionando conforme o planejado, fornecendo respostas relevantes, precisas e adaptadas ao perfil do usuário. ### 6.3. Testes de Funcionalidade Os **testes de funcionalidade** serão realizados para verificar se todos os requisitos funcionais definidos para o site e para o chatbot foram implementados corretamente. Esses testes se concentram no comportamento externo do sistema, garantindo que cada funcionalidade opere conforme o esperado. Para o site, os testes incluirão a verificação da navegação, a exibição correta de todas as páginas estáticas, a funcionalidade do link para a tese completa e a integração da interface do chatbot. Para o chatbot, os testes abrangerão a seleção de personas, a capacidade de receber perguntas e gerar respostas, a correta aplicação da linguagem de cada persona e a integração com o sistema RAG e o LLM Kimie K2 via OpenRouter. Serão utilizados casos de teste elaborados a partir dos requisitos funcionais (Seção 3.5) e dos critérios de aceitação (Seções 6.1 e 6.2). Os testes serão realizados manualmente, simulando a interação de um usuário final com o sistema. Qualquer defeito ou comportamento inesperado identificado será registrado e corrigido antes da fase de Confirmação. A cobertura dos testes deve ser abrangente, garantindo que todas as funcionalidades principais tenham sido validadas. ### 6.4. Testes de Usabilidade Os **testes de usabilidade** terão como objetivo avaliar a facilidade de uso, a eficiência e a satisfação do usuário ao interagir com o site e o chatbot modernizados. Esses testes serão conduzidos com representantes do público-alvo, sempre que possível, ou com membros da equipe que não estiveram diretamente envolvidos no desenvolvimento, para obter feedback imparcial. Para o site, os testes de usabilidade se concentrarão na clareza da navegação, na legibilidade do conteúdo e na facilidade de encontrar informações. Para o chatbot, os testes avaliarão a intuitividade da interface de chat, a clareza das opções de persona, a naturalidade da conversa e a compreensibilidade das respostas fornecidas por ambas as personas ("Dr. Gasnelio" e "Gá"). Serão observados aspectos como a curva de aprendizado, a ocorrência de erros durante a interação e a satisfação geral do usuário. Tarefas específicas serão definidas para os testadores executarem, como encontrar uma informação específica no site ou obter uma explicação sobre um conceito através do chatbot. O feedback coletado será utilizado para identificar áreas de melhoria e realizar ajustes finos na interface e na experiência do usuário antes da implantação final. A usabilidade é particularmente importante para garantir que o chatbot atenda efetivamente tanto ao público técnico quanto ao leigo. ### 6.5. Verificação de Coerência das Personas A **verificação de coerência das personas** é um aspecto crítico da validação do chatbot, garantindo que as personalidades "Dr. Gasnelio" (técnica) e "Gá" (leiga) se comportem de maneira consistente e distinta, conforme definido na Seção 3.4. Este processo envolverá a realização de uma série de perguntas-teste para ambas as personas, cobrindo uma variedade de tópicos relacionados à tese de doutorado e ao roteiro de dispensação. As respostas geradas serão analisadas quanto ao **tom de voz, nível de formalidade, complexidade da linguagem, uso de jargões técnicos e empatia**. Por exemplo, espera-se que o "Dr. Gasnelio" utilize uma linguagem culta, objetiva e precise, enquanto "Gá" deve usar uma linguagem simples, coloquial, acolhedora e, quando apropriado, bem-humorada. A avaliação será feita por membros da equipe de desenvolvimento e, se possível, por stakeholders que possam fornecer feedback sobre a adequação das personas aos seus respectivos públicos-alvo. Serão verificados se há vazamentos de estilo entre as personas (ex: "Gá" usando termos muito técnicos sem explicação) ou inconsistências no comportamento ao longo da conversa. A eficácia da injeção dinâmica de `PromptTemplates` para controlar o comportamento do LLM será um foco principal desta verificação. Qualquer inconsistência identificada exigirá ajustes nos `PromptTemplates` ou na lógica de seleção de persona no backend. ## 7. Confirmação (C) ### 7.1. Verificação Final do Sistema em Produção A **verificação final do sistema em produção** é a última etapa de testes antes de considerar o projeto como concluído e entregue. Nesta fase, o sistema modernizado, incluindo o site e o chatbot, será testado em seu ambiente de produção real na plataforma Render. O objetivo é garantir que todos os componentes estejam funcionando corretamente como um todo integrado, sem os emuladores ou ambientes de teste locais. Serão repetidos os principais testes de funcionalidade e usabilidade realizados na fase de Validação, mas agora diretamente no ambiente de produção. Isso inclui testar a navegação do site, a interação com o chatbot, a seleção de personas, a geração de respostas baseadas em RAG e a integração com a OpenRouter. A equipe verificará se todas as configurações de produção, como variáveis de ambiente, Secret Files e parâmetros do `render.yaml`, estão corretas e se o sistema se comporta conforme o esperado sob condições reais de acesso. Qualquer problema identificado nesta etapa final deve ser tratado com prioridade antes de prosseguir para as demais atividades de confirmação. Esta verificação assegura que o produto entregue aos usuários finais seja estável e confiável. ### 7.2. Revisão de Segurança dos Dados A **revisão de segurança dos dados** é uma etapa crucial para garantir a proteção de informações sensíveis e a conformidade com boas práticas de segurança. Neste projeto, o foco principal da segurança recai sobre o gerenciamento das chaves de API, como a chave da OpenRouter utilizada para acessar o modelo Kimie K2. A verificação confirmará que essas chaves estão sendo armazenadas e gerenciadas de forma segura através dos **Secret Files do Render**, e que não há vazamento dessas informações no código-fonte, repositório Git ou logs do sistema. Além disso, será verificada a segurança da comunicação entre os diferentes componentes do sistema, especialmente as chamadas de API entre o backend Python, o LangFlow (se aplicável como serviço separado) e a OpenRouter, assegurando que ocorram preferencialmente sobre HTTPS. Se o banco de dados vetorial Astra DB for utilizado, sua configuração de segurança e controle de acesso também será revisada. Embora a base de conhecimento (tese de doutorado) seja um documento público, a revisão de segurança garantirá que o sistema não exponha inadvertidamente dados internos, informações de depuração sensíveis ou detalhes da infraestrutura que possam ser explorados. A equipe também deve considerar a implementação de práticas básicas de segurança em nível de aplicação, como validação de entrada e proteção contra ataques comuns, dentro do possível considerando a premissa de gratuidade e o escopo do projeto. ### 7.3. Documentação de Entrega A **documentação de entrega** é essencial para formalizar o término do projeto e fornecer informações relevantes para futuras manutenções ou evoluções do sistema. Esta documentação incluirá, no mínimo: * **Este Documento de Requisitos do Produto (PRD) finalizado**, contendo todas as especificações, arquitetura, tecnologias e critérios de aceitação. * **Manual do Usuário (ou Guia Rápido)**: Instruções simples sobre como acessar e utilizar o site e o chatbot, incluindo a seleção de personas. * **Documentação Técnica do Desenvolvimento**: * Descrição detalhada da arquitetura implementada. * Instruções para configuração do ambiente de desenvolvimento local e de produção (incluindo dependências e variáveis de ambiente). * Diagramas atualizados de arquitetura e fluxo de dados. * Descrição das APIs internas e externas utilizadas. * Estrutura do repositório Git e fluxo de trabalho de desenvolvimento. * Detalhes sobre a implementação do RAG no LangFlow e a integração com o Kimie K2 via OpenRouter. * **Relatório de Testes**: Sumário dos testes realizados, resultados obtidos e eventuais problemas encontrados e corrigidos. * **Lista de Pendências ou Melhorias Futuras**: Identificação de funcionalidades que não foram implementadas, limitações conhecidas ou sugestões para futuras versões. Esta documentação deve ser armazenada de forma organizada, preferencialmente no repositório Git do projeto ou em local de fácil acesso para a equipe e stakeholders relevantes. A clareza e a completude da documentação de entrega são fundamentais para a sustentabilidade do projeto a longo prazo. ### 7.4. Preparação para Escalabilidade A **preparação para escalabilidade** envolve considerar, desde o projeto e implementação, como o sistema poderia ser expandido ou adaptado para lidar com um aumento de carga de usuários, volume de dados ou novas funcionalidades no futuro. Embora o projeto atual tenha a premissa de custo zero e, portanto, utilize planos gratuitos com limitações de recursos, a arquitetura deve ser pensada de forma a facilitar uma eventual migração para planos pagos ou a otimização para melhor desempenho. Para o site e o backend Python no Render, isso significa estruturar o código de forma modular e considerar a possibilidade de utilizar serviços com mais recursos ou balanceamento de carga, se necessário. Para o chatbot, a escolha do banco de dados vetorial (FAISS localmente ou Astra DB gerenciado) tem implicações diretas na escalabilidade. Se FAISS for usado, sua eficiência em termos de memória e CPU será crucial. Se Astra DB for escolhido, sua natureza gerenciada e escalável pode oferecer vantagens. O pipeline RAG no LangFlow deve ser projetado para ser eficiente, e a integração com o LLM via OpenRouter deve considerar os limites de taxa e a possibilidade de otimização de prompts. Além disso, a preparação para a extensão do escopo das doenças contempladas no roteiro, conforme mencionado nas diretrizes, é uma forma de escalabilidade funcional. A base de conhecimento e o pipeline RAG devem ser projetados para facilitar a inclusão de novos dados e documentos sem grandes refatorações. A documentação do código e da arquitetura também contribui para a escalabilidade, facilitando a compreensão e a modificação do sistema por outras equipes no futuro. ## 8. Considerações Adicionais ### 8.1. Limitações Técnicas O projeto, por se tratar de uma iniciativa sem custos financeiros, está sujeito a uma série de **limitações técnicas decorrentes da utilização de serviços e ferramentas em suas camadas gratuitas ou de código aberto**. A plataforma de hospedagem **Render**, embora ofereça um tier gratuito, possui restrições quanto a recursos de CPU, RAM, tempo de atividade e largura de banda. Isso pode impactar o desempenho do site e do chatbot, especialmente sob carga de usuários mais alta ou durante processamentos intensivos. Por exemplo, serviços web no plano gratuito do Render podem entrar em sono após um período de inatividade, causando um atraso no primeiro carregamento. Além disso, a escalabilidade é limitada nesse plano. A **OpenRouter**, plataforma utilizada para acessar o modelo Kimie K2 de forma gratuita, também impõe limites de uso. Esses limites podem incluir um número máximo de requisições por dia, tokens processados por minuto ou por mês, e possivelmente uma fila compartilhada que pode afetar a latência durante picos de uso . O modelo **Kimie K2 Free** acessado via OpenRouter pode ter algumas funcionalidades limitadas em comparação com versões pagas ou de auto-hospedagem, como suporte a ferramentas/plugins ou recursos multimodais completos . A latência da resposta do Kimie K2 via OpenRouter também pode variar dependendo da carga da plataforma e da localização dos servidores. Se optarmos por um banco de dados vetorial como o **Astra DB**, sua camada gratuita (se disponível) também terá limitações de armazenamento, throughput e funcionalidades. Se a opção for o **FAISS** auto-hospedado, a limitação estará nos recursos da máquina onde ele é executado (provavelmente a mesma do serviço web no Render, compartilhando seus limites de CPU e RAM). O **LangFlow**, sendo uma ferramenta para construção de fluxos, depende dos recursos da máquina onde é executado e da eficiência dos componentes LLM e de banco de dados vetorial integrados. A complexidade do fluxo RAG e o tamanho da base de conhecimento (tese de doutorado) podem demandar um tempo considerável para processamento inicial (criação de embeddings e índice) e para consultas, o que deve ser monitorado dentro dos limites do Render. A gratuidade do projeto também limita a capacidade de investir em soluções de monitoramento mais sofisticadas ou em ferramentas pagas que poderiam otimizar o desempenho ou a confiabilidade. Portanto, é crucial gerenciar as expectativas dos stakeholders em relação à disponibilidade, desempenho e escalabilidade do sistema, comunicando claramente essas limitações inerentes à abordagem de custo zero. A escolha cuidadosa das tecnologias e a otimização do código e dos fluxos serão essenciais para maximizar a eficiência dentro dessas restrições. ### 8.2. Dependências O sucesso deste projeto depende de uma série de fatores externos e internos. A compreensão e o gerenciamento dessas dependências são cruciais para o planejamento e a execução eficazes. **Dependências Externas:** 1. **Disponibilidade e Estabilidade de Serviços de Terceiros**: * **Render**: A estabilidade e a continuidade do plano gratuito da plataforma Render são essenciais para a hospedagem do site e do backend. * **OpenRouter**: O acesso gratuito e contínuo ao modelo Kimie K2 através da OpenRouter é fundamental para o funcionamento do chatbot. Mudanças nas políticas, limites de uso ou disponibilidade da OpenRouter podem impactar o projeto. * **Hugging Face**: A disponibilidade dos modelos de embedding e das bibliotecas do Hugging Face é necessária para o processamento da base de conhecimento e das consultas do RAG. * **LangFlow e LangChain**: A estabilidade e a manutenção contínua dessas ferramentas de código aberto são importantes para o desenvolvimento e a manutenção do pipeline RAG. * **FAISS / Astra DB**: A disponibilidade e o desempenho da biblioteca FAISS ou do serviço Astra DB (se escolhido) são críticos para a funcionalidade de busca vetorial. 2. **Acesso à Base de Conhecimento**: A tese de doutorado, que serve como base de conhecimento, deve estar acessível e em um formato que possa ser processado pelas ferramentas escolhidas (ex: PDF, Markdown). **Dependências Internas:** 1. **Recursos da Equipe**: A disponibilidade e a expertise da equipe de desenvolvimento em Python, ferramentas de IA, LangFlow, Render e gerenciamento de projetos. 2. **Acesso ao Repositório Original (se necessário)**: Se for decidido clonar arquivos do repositório original, o acesso a esse repositório (atualmente não acessível ) se torna uma dependência. 3. **Definição Clara de Personas e Requisitos**: A clareza e a estabilidade dos requisitos, especialmente das personas do chatbot e do escopo do site, são fundamentais para evitar retrabalho. 4. **Tempo e Esforço**: A capacidade de dedicar tempo e esforço suficientes para o desenvolvimento, testes e implantação dentro do prazo esperado (um mês para um site de poucas páginas e um chatbot funcional). O gerenciamento proativo dessas dependências, com planos de contingência para possíveis falhas ou indisponibilidades, será importante para o andamento suave do projeto. ### 8.3. Instruções para Replicação do Ambiente Local Para que a equipe de desenvolvimento possa trabalhar de forma eficiente e colaborativa, é essencial fornecer instruções claras para a replicação do ambiente de desenvolvimento local. Essas instruções garantirão que todos os membros da equipe tenham uma configuração consistente, minimizando problemas de "funciona na minha máquina". **Instruções para Replicação do Ambiente Local:** 1. **Pré-requisitos de Sistema**: * **Python**: Instalar a versão de Python especificada para o projeto (ex: Python 3.9 ou superior). Recomenda-se o uso de um gerenciador de versões como `pyenv` (para Linux/macOS) ou a instalação direta do Python.org (para Windows). * **Git**: Instalar o Git para controle de versão. * **Pip**: Gerenciador de pacotes Python (geralmente vem com a instalação do Python). 2. **Clonar o Repositório**: * Abra um terminal ou prompt de comando. * Navegue até o diretório onde deseja clonar o projeto. * Execute o comando: `git clone https://github.com/seu-usuario/roteiro-de-dispersacao-v4.git` * Navegue para o diretório do projeto: `cd roteiro-de-dispersacao-v4` 3. **Configurar Ambiente Virtual (Recomendado)**: * É altamente recomendável utilizar um ambiente virtual para isolar as dependências do projeto. * Para criar um ambiente virtual (ex: com `venv`): ```bash python -m venv venv ``` * Para ativar o ambiente virtual: * No Linux/macOS: `source venv/bin/activate` * No Windows (PowerShell): `.\venv\Scripts\Activate.ps1` * No Windows (CMD): `.\venv\Scripts\activate.bat` 4. **Instalar Dependências**: * Com o ambiente virtual ativado, instale as dependências listadas no arquivo `requirements.txt` (ou `pyproject.toml` se estiver usando Poetry): ```bash pip install -r requirements.txt ``` Ou, se estiver usando `pyproject.toml` (com Poetry): ```bash poetry install ``` 5. **Configurar Variáveis de Ambiente Locais**: * Crie um arquivo `.env` na raiz do projeto para armazenar variáveis de ambiente locais (similar ao que será feito com Secret Files no Render). * Adicione as variáveis necessárias, como chaves de API (ex: `OPENROUTER_API_KEY=suachaveaqui`). **NUNCA** faça commit deste arquivo `.env` para o repositório Git. * Certifique-se de que a aplicação Python esteja configurada para ler as variáveis deste arquivo. 6. **Executar a Aplicação Localmente**: * Dependendo da estrutura do projeto e do framework web escolhido (Flask/FastAPI), siga as instruções específicas para executar o servidor de desenvolvimento local. * Por exemplo, para Flask, pode ser um comando como: ```bash flask run ``` * Para Streamlit, se estiver sendo usado para o frontend do chatbot: ```bash streamlit run app_chatbot.py ``` * Acesse a aplicação no navegador através do endereço e porta indicados (ex: `http://localhost:5000` ou `http://localhost:8501`). 7. **Configurar LangFlow Localmente (se aplicável)**: * Se o LangFlow for executado localmente para desenvolvimento e teste do fluxo RAG, siga as instruções de instalação e execução da documentação oficial do LangFlow. * Isso pode envolver a instalação do LangFlow via pip e a execução de um comando como `langflow run`. 8. **Banco de Dados Vetorial Local (FAISS)**: * Se FAISS for usado localmente, siga as instruções de instalação da biblioteca FAISS e de como carregar/salvar os índices gerados a partir da base de conhecimento. Essas instruções devem ser mantidas atualizadas no `README.md` do repositório do projeto para facilitar a integração de novos membros da equipe e garantir a consistência do ambiente de desenvolvimento. ### 8.4. Controle de Versão O **controle de versão** será gerenciado utilizando o sistema **Git**, com o repositório do projeto hospedado no **GitHub**. Um novo repositório, denominado `roteiro-de-dispersacao-v4`, será criado especificamente para esta modernização. A decisão de criar um novo repositório visa ter um histórico limpo e focado nas novas implementações, especialmente considerando a possível indisponibilidade ou privacidade do repositório original . Se, eventualmente, o acesso ao repositório original for obtido, apenas os arquivos essenciais para o funcionamento do site e do chatbot serão considerados para clonagem ou migração, evitando a importação de histórico desnecessário ou de arquivos obsoletos. **Estratégia de Branching e Fluxo de Trabalho:** * **Branch Principal**: A branch principal (geralmente `main` ou `master`) conterá o código estável e implantável em produção. * **Branches de Desenvolvimento/Feature**: * Para cada nova funcionalidade, correção de bug ou tarefa significativa, uma nova branch de feature será criada a partir da branch principal. O nome da branch deve ser descritivo (ex: `feature/integracao-openrouter`, `fix/correcao-navbar`, `refactor/backend-api`). * O desenvolvimento da funcionalidade ocorrerá nesta branch de feature. * **Pull Requests (PRs) / Merge Requests (MRs)**: * Quando uma funcionalidade estiver concluída e testada localmente na branch de feature, um Pull Request será aberto para mesclar as alterações na branch principal. * O PR deve conter uma descrição clara das alterações realizadas. * O código do PR deve ser revisado por pelo menos um outro membro da equipe antes da aprovação e merge. * A integração contínua (CI) configurada no Render (acionada pelo `render.yaml`) pode ser configurada para rodar testes automatizados sempre que um PR for aberto ou atualizado, fornecendo feedback rápido sobre a integridade do código. * **Commits Significativos**: Os commits devem ser atômicos e conter mensagens descritivas do que foi alterado e por quê. * **Tags/Releases**: Para marcar versões significativas do projeto (ex: versão 1.0.0 de lançamento), podem ser criadas tags no Git. Esta estratégia de controle de versão visa garantir um histórico de código organizado, facilitar a colaboração, permitir o rastreamento de alterações e integrar-se com o pipeline de CI/CD no Render para implantações automatizadas e seguras.