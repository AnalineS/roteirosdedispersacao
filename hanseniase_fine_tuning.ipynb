{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/user/hanseniase-fine-tuning/blob/main/hanseniase_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# üè• Fine-tuning de Modelo M√©dico para Hansen√≠ase\n",
        "## Treinamento especializado para as personas Dr. Gasnelio e G√°\n",
        "\n",
        "Este notebook realiza o fine-tuning de um modelo biom√©dico para o sistema de dispensa√ß√£o de medicamentos para hansen√≠ase, com duas personas especializadas:\n",
        "\n",
        "- **Dr. Gasnelio**: Farmac√™utico t√©cnico com respostas cient√≠ficas e precisas\n",
        "- **G√°**: Assistente emp√°tico com linguagem simples e acolhedora\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üì¶ 1. Setup do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias necess√°rias\n",
        "!pip install transformers==4.36.0\n",
        "!pip install peft==0.7.0\n",
        "!pip install bitsandbytes==0.41.0\n",
        "!pip install datasets accelerate\n",
        "!pip install sentencepiece protobuf\n",
        "!pip install torch torchaudio torchvision\n",
        "!pip install wandb  # Para logging opcional\n",
        "!pip install evaluate  # Para m√©tricas de avalia√ß√£o\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Verificar GPU dispon√≠vel\n",
        "import torch\n",
        "print(f\"üöÄ GPU dispon√≠vel: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üî• GPU Nome: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n",
        "    print(\"‚ö†Ô∏è  Nenhuma GPU encontrada. O treinamento ser√° mais lento em CPU.\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üéØ Dispositivo selecionado: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "## üíæ 2. Montagem do Drive e Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Definir caminhos\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Site roteiro de dispensa√ß√£o'\n",
        "DATA_PATH = f'{DRIVE_PATH}/training_data.json'\n",
        "SPLITS_PATH = f'{DRIVE_PATH}/training_splits'\n",
        "\n",
        "# Adicionar ao path para importa√ß√µes\n",
        "sys.path.append(DRIVE_PATH)\n",
        "\n",
        "print(f\"üìÅ Caminho base: {DRIVE_PATH}\")\n",
        "print(f\"üìä Dados de treinamento: {DATA_PATH}\")\nprint(f\"üîÑ Splits: {SPLITS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Carregar dados de treinamento\n",
        "try:\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        training_data = json.load(f)\n",
        "    \n",
        "    # Carregar splits individuais\n",
        "    splits = {}\n",
        "    for split_name in ['train', 'validation', 'test']:\n",
        "        split_file = f'{SPLITS_PATH}/{split_name}.json'\n",
        "        if os.path.exists(split_file):\n",
        "            with open(split_file, 'r', encoding='utf-8') as f:\n",
        "                splits[split_name] = json.load(f)\n",
        "    \n",
        "    # Exibir estat√≠sticas\n",
        "    total_examples = training_data['statistics']['total_examples']\n",
        "    augmented_total = training_data.get('augmented_total', total_examples)\n",
        "    \n",
        "    print(f\"üìà Total de exemplos originais: {total_examples}\")\n",
        "    print(f\"üîÑ Total ap√≥s data augmentation: {augmented_total}\")\n",
        "    \n",
        "    print(\"\\nüìä Distribui√ß√£o por categoria:\")\n",
        "    for category, count in training_data['statistics']['examples_by_category'].items():\n",
        "        print(f\"  ‚Ä¢ {category}: {count} exemplos\")\n",
        "    \n",
        "    print(\"\\nüé≠ Distribui√ß√£o por persona:\")\n",
        "    for persona, percentage in training_data['statistics']['persona_distribution'].items():\n",
        "        print(f\"  ‚Ä¢ {persona}: {percentage:.1f}%\")\n",
        "    \n",
        "    print(\"\\nüîÑ Splits:\")\n",
        "    for split_name, split_data in splits.items():\n",
        "        print(f\"  ‚Ä¢ {split_name}: {len(split_data)} exemplos\")\n",
        "        \n",
        "    print(\"\\n‚úÖ Dados carregados com sucesso!\")\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
        "    print(\"\\nüí° Certifique-se de que os arquivos est√£o no Google Drive:\")\n",
        "    print(f\"   - {DATA_PATH}\")\n",
        "    print(f\"   - {SPLITS_PATH}/train.json\")\n",
        "    print(f\"   - {SPLITS_PATH}/validation.json\")\n",
        "    print(f\"   - {SPLITS_PATH}/test.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep"
      },
      "source": [
        "## üîß 3. Prepara√ß√£o dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format_data"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\n",
        "\n",
        "def format_instruction(example):\n",
        "    \"\"\"\n",
        "    Formatar exemplos no padr√£o instruction-following para fine-tuning\n",
        "    \"\"\"\n",
        "    instruction = example['instruction']\n",
        "    input_text = example['input']\n",
        "    output_text = example['output']\n",
        "    \n",
        "    # Formato otimizado para modelos m√©dicos\n",
        "    if input_text.strip():\n",
        "        # Quando h√° input (pergunta)\n",
        "        prompt = f\"\"\"### Instru√ß√£o:\n",
        "{instruction}\n",
        "\n",
        "### Pergunta:\n",
        "{input_text}\n",
        "\n",
        "### Resposta:\n",
        "{output_text}\"\"\"\n",
        "    else:\n",
        "        # Quando √© apenas instru√ß√£o\n",
        "        prompt = f\"\"\"### Instru√ß√£o:\n",
        "{instruction}\n",
        "\n",
        "### Resposta:\n",
        "{output_text}\"\"\"\n",
        "    \n",
        "    return {'text': prompt}\n",
        "\n",
        "# Preparar datasets\n",
        "print(\"üîÑ Formatando dados para fine-tuning...\")\n",
        "\n",
        "datasets_dict = {}\n",
        "for split_name, split_data in splits.items():\n",
        "    # Converter para Dataset do Hugging Face\n",
        "    dataset = Dataset.from_list(split_data)\n",
        "    \n",
        "    # Aplicar formata√ß√£o\n",
        "    formatted_dataset = dataset.map(format_instruction)\n",
        "    \n",
        "    datasets_dict[split_name] = formatted_dataset\n",
        "    print(f\"  ‚úÖ {split_name}: {len(formatted_dataset)} exemplos formatados\")\n",
        "\n",
        "# Criar DatasetDict\n",
        "dataset_dict = DatasetDict(datasets_dict)\n",
        "\n",
        "print(\"\\nüìù Exemplo de entrada formatada:\")\nprint(\"=\"*60)\nprint(dataset_dict['train'][0]['text'][:500] + \"...\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_config"
      },
      "source": [
        "## üß† 4. Configura√ß√£o do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Modelo base otimizado para tarefas m√©dicas\n",
        "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "# Alternativa para modelos mais gerais: \"microsoft/DialoGPT-medium\"\n",
        "# Alternativa para portugu√™s: \"neuralmind/bert-base-portuguese-cased\"\n",
        "\n",
        "print(f\"üß† Carregando modelo: {MODEL_NAME}\")\n",
        "\n",
        "# Configura√ß√£o para quantiza√ß√£o (economia de mem√≥ria)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Carregar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Carregar modelo\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Preparar modelo para treinamento\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(f\"‚úÖ Modelo carregado com {model.num_parameters():,} par√¢metros\")\nprint(f\"üéØ Tokenizer configurado com {len(tokenizer)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lora_config"
      },
      "outputs": [],
      "source": [
        "# Configura√ß√£o LoRA (Low-Rank Adaptation)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # Rank - balance entre performance e efici√™ncia\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    target_modules=[         # M√≥dulos a serem fine-tuned\n",
        "        \"q_proj\",\n",
        "        \"k_proj\", \n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.1,       # Dropout para regulariza√ß√£o\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        ")\n",
        "\n",
        "# Aplicar LoRA ao modelo\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Exibir par√¢metros trein√°veis\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\npercentage = 100 * trainable_params / all_param\n",
        "print(f\"üéØ Par√¢metros trein√°veis: {trainable_params:,} / {all_param:,} ({percentage:.2f}%)\")\n",
        "print(f\"üíæ Economia de mem√≥ria: {100-percentage:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization"
      },
      "source": [
        "## üî§ 5. Tokeniza√ß√£o dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenize"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizar textos com configura√ß√µes otimizadas para fine-tuning m√©dico\n",
        "    \"\"\"\n",
        "    # Tokenizar com padding e truncation\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,  # Ajustar conforme necess√°rio\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    # Para causal LM, labels s√£o os pr√≥prios input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Aplicar tokeniza√ß√£o\n",
        "print(\"üî§ Tokenizando datasets...\")\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text'],  # Remover colunas originais\n",
        "    desc=\"Tokenizing datasets\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tokeniza√ß√£o conclu√≠da!\")\n",
        "print(f\"üìä Shape do dataset de treino: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"üìè Comprimento m√°ximo dos tokens: {tokenized_datasets['train']['input_ids'][0].shape}\")\n",
        "\n",
        "# Verificar exemplo tokenizado\n",
        "sample = tokenized_datasets['train'][0]\nprint(f\"\\nüîç Exemplo tokenizado:\")\nprint(f\"  ‚Ä¢ Input IDs shape: {len(sample['input_ids'])}\")\nprint(f\"  ‚Ä¢ Labels shape: {len(sample['labels'])}\")\nprint(f\"  ‚Ä¢ Attention mask shape: {len(sample['attention_mask'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## üöÄ 6. Configura√ß√£o e Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_args"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "import wandb\n",
        "\n",
        "# Configura√ß√µes de treinamento otimizadas para Colab Free\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./hanseniase-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    \n",
        "    # Configura√ß√µes de epochs e batch\n",
        "    num_train_epochs=3,              # Ajustar conforme necess√°rio\n",
        "    per_device_train_batch_size=2,   # Reduzido para economizar mem√≥ria\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,   # Simular batch size maior\n",
        "    \n",
        "    # Otimizador e learning rate\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    \n",
        "    # Logging e avalia√ß√£o\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    \n",
        "    # Economia de mem√≥ria\n",
        "    fp16=False,                      # Usar se GPU suportar\n",
        "    bf16=True,                       # Melhor para modelos maiores\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Early stopping\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # Outros\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # Desabilitar wandb se n√£o quiser logging\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Data collator para language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # N√£o usar masked language modeling\n",
        ")\n",
        "\n",
        "print(\"‚öôÔ∏è Configura√ß√µes de treinamento definidas:\")\nprint(f\"  ‚Ä¢ Epochs: {training_args.num_train_epochs}\")\nprint(f\"  ‚Ä¢ Batch size por device: {training_args.per_device_train_batch_size}\")\nprint(f\"  ‚Ä¢ Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  ‚Ä¢ Learning rate: {training_args.learning_rate}\")\nprint(f\"  ‚Ä¢ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trainer"
      },
      "outputs": [],
      "source": [
        "# Criar trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"üèÉ‚Äç‚ôÇÔ∏è Trainer criado e pronto para treinamento!\")\nprint(f\"üìä Dataset de treino: {len(tokenized_datasets['train'])} exemplos\")\nprint(f\"üîç Dataset de valida√ß√£o: {len(tokenized_datasets['validation'])} exemplos\")\n",
        "\n",
        "# Verificar mem√≥ria dispon√≠vel\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüíæ Mem√≥ria GPU:\")\n",
        "    print(f\"  ‚Ä¢ Alocada: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "    print(f\"  ‚Ä¢ Em cache: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "    print(f\"  ‚Ä¢ Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_training"
      },
      "outputs": [],
      "source": [
        "# Iniciar treinamento\n",
        "print(\"üöÄ Iniciando fine-tuning...\")\nprint(\"‚è≥ Isto pode levar v√°rios minutos dependendo do tamanho do dataset e configura√ß√µes.\")\nprint(\"\\n\" + \"=\"*60)\n",
        "\ntry:\n",
        "    # Treinar modelo\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéâ Treinamento conclu√≠do com sucesso!\")\n",
        "    \n",
        "    # Salvar modelo\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(training_args.output_dir)\n",
        "    \n",
        "    print(f\"üíæ Modelo salvo em: {training_args.output_dir}\")\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"\\n‚ùå Erro durante treinamento: {e}\")\n",
        "    print(\"\\nüí° Poss√≠veis solu√ß√µes:\")\n",
        "    print(\"  1. Reduzir batch_size\")\n",
        "    print(\"  2. Reduzir max_length\")\n",
        "    print(\"  3. Usar gradient_checkpointing=True\")\n",
        "    print(\"  4. Verificar se GPU tem mem√≥ria suficiente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## üìä 7. Avalia√ß√£o do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Avaliar modelo no conjunto de teste\n",
        "print(\"üìä Avaliando modelo no conjunto de teste...\")\n",
        "\ntry:\n",
        "    eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "    \n",
        "    print(\"\\nüìà Resultados da avalia√ß√£o:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"  ‚Ä¢ {key}: {value:.4f}\")\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"‚ùå Erro na avalia√ß√£o: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_inference"
      },
      "outputs": [],
      "source": [
        "# Testar infer√™ncia com exemplos espec√≠ficos\n",
        "print(\"üß™ Testando infer√™ncia com exemplos espec√≠ficos...\")\n",
        "\n",
        "# Criar pipeline de gera√ß√£o de texto\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Exemplos de teste para cada persona\n",
        "test_prompts = [\n",
        "    {\n",
        "        \"prompt\": \"\"\"### Instru√ß√£o:\nComo Dr. Gasnelio, responda tecnicamente:\n\n### Pergunta:\nQual √© o mecanismo de a√ß√£o da rifampicina?\n\n### Resposta:\"\"\",\n",
        "        \"persona\": \"Dr. Gasnelio\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"\"\"### Instru√ß√£o:\nComo G√°, responda de forma simples e emp√°tica:\n\n### Pergunta:\nPor que minha urina ficou laranja?\n\n### Resposta:\"\"\",\n",
        "        \"persona\": \"G√°\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"\"\"### Instru√ß√£o:\nExplique o protocolo de dosagem supervisionada de clofazimina para adultos:\n\n### Resposta:\"\"\",\n",
        "        \"persona\": \"T√©cnico\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\nfor i, test in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüé≠ Teste {i} - Persona: {test['persona']}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Gerar resposta\n",
        "        result = generator(test['prompt'], max_new_tokens=100, num_return_sequences=1)\n",
        "        generated_text = result[0]['generated_text']\n",
        "        \n",
        "        # Extrair apenas a resposta gerada (ap√≥s \"### Resposta:\")\n",
        "        response_start = generated_text.find(\"### Resposta:\") + len(\"### Resposta:\")\n",
        "        response = generated_text[response_start:].strip()\n",
        "        \n",
        "        print(f\"üí¨ Resposta gerada:\")\n",
        "        print(f\"{response}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro na gera√ß√£o: {e}\")\n",
        "\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Testes de infer√™ncia conclu√≠dos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## üíæ 8. Export e Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_weights"
      },
      "outputs": [],
      "source": [
        "# Merge LoRA weights com modelo base\n",
        "print(\"üîó Fazendo merge dos weights LoRA...\")\n",
        "\ntry:\n",
        "    # Merge dos weights\n",
        "    merged_model = model.merge_and_unload()\n",
        "    \n",
        "    # Salvar modelo final\n",
        "    final_model_path = \"./hanseniase-final-model\"\n",
        "    merged_model.save_pretrained(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "    \n",
        "    print(f\"‚úÖ Modelo final salvo em: {final_model_path}\")\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"‚ùå Erro no merge: {e}\")\n",
        "    print(\"üí° Usando modelo com LoRA adapters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_to_drive"
      },
      "outputs": [],
      "source": [
        "# Salvar modelo no Google Drive\nprint(\"üíæ Salvando modelo treinado no Google Drive...\")\n",
        "\nimport shutil\n",
        "\ntry:\n",
        "    # Criar diret√≥rio no Drive\n",
        "    drive_model_path = f'{DRIVE_PATH}/hanseniase_fine_tuned_model'\n",
        "    os.makedirs(drive_model_path, exist_ok=True)\n",
        "    \n",
        "    # Copiar arquivos do modelo\n",
        "    model_files = [\n",
        "        'config.json',\n",
        "        'generation_config.json', \n",
        "        'pytorch_model.bin',\n",
        "        'tokenizer.json',\n",
        "        'tokenizer_config.json',\n",
        "        'vocab.txt'\n",
        "    ]\n",
        "    \n",
        "    source_dir = training_args.output_dir\n",
        "    \n",
        "    for file in model_files:\n",
        "        source_file = os.path.join(source_dir, file)\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy2(source_file, drive_model_path)\n",
        "            print(f\"  ‚úÖ {file} copiado\")\n",
        "    \n",
        "    # Salvar m√©tricas de treinamento\n",
        "    metrics_file = f'{drive_model_path}/training_metrics.json'\n",
        "    training_metrics = {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'training_examples': len(tokenized_datasets['train']),\n",
        "        'validation_examples': len(tokenized_datasets['validation']),\n",
        "        'test_examples': len(tokenized_datasets['test']),\n",
        "        'epochs': training_args.num_train_epochs,\n",
        "        'learning_rate': training_args.learning_rate,\n",
        "        'batch_size': training_args.per_device_train_batch_size,\n",
        "        'lora_rank': lora_config.r,\n",
        "        'lora_alpha': lora_config.lora_alpha,\n",
        "        'trainable_parameters': trainable_params,\n",
        "        'total_parameters': all_param,\n",
        "        'trainable_percentage': percentage\n",
        "    }\n",
        "    \n",
        "    with open(metrics_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_metrics, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nüéâ Modelo e m√©tricas salvos com sucesso!\")\n",
        "    print(f\"üìÅ Localiza√ß√£o: {drive_model_path}\")\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"‚ùå Erro ao salvar no Drive: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "integration"
      },
      "source": [
        "## üîå 9. Instru√ß√µes de Integra√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "integration_guide"
      },
      "outputs": [],
      "source": [
        "# Gerar instru√ß√µes de integra√ß√£o\nintegration_guide = f\"\"\"\n# üîå Guia de Integra√ß√£o - Modelo Fine-tuned para Hansen√≠ase\n\n## üìã Informa√ß√µes do Modelo\n\n- **Modelo Base:** {MODEL_NAME}\n- **Exemplos de Treinamento:** {len(tokenized_datasets['train'])}\n- **Par√¢metros Trein√°veis:** {trainable_params:,} ({percentage:.2f}%)\n- **LoRA Rank:** {lora_config.r}\n- **Epochs:** {training_args.num_train_epochs}\n\n## üöÄ Como Usar no Backend\n\n### 1. Instala√ß√£o das Depend√™ncias\n\n```bash\npip install transformers==4.36.0\npip install peft==0.7.0\npip install torch\n```\n\n### 2. Carregamento do Modelo\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Carregar modelo base\nbase_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{MODEL_NAME}\")\n\n# Carregar adaptadores LoRA\nmodel = PeftModel.from_pretrained(base_model, \"./hanseniase-finetuned\")\n\n# Fazer merge (opcional, para melhor performance)\nmodel = model.merge_and_unload()\n```\n\n### 3. Fun√ß√£o de Infer√™ncia\n\n```python\ndef generate_hanseniase_response(instruction, input_text=\"\", persona=\"both\"):\n    # Formatar prompt\n    if input_text.strip():\n        prompt = f\"\"\"### Instru√ß√£o:\n{{instruction}}\n\n### Pergunta:\n{{input_text}}\n\n### Resposta:\"\"\"\n    else:\n        prompt = f\"\"\"### Instru√ß√£o:\n{{instruction}}\n\n### Resposta:\"\"\"\n    \n    # Tokenizar\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    \n    # Gerar resposta\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decodificar\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extrair apenas a resposta\n    response_start = generated_text.find(\"### Resposta:\") + len(\"### Resposta:\")\n    response = generated_text[response_start:].strip()\n    \n    return response\n```\n\n### 4. Integra√ß√£o com Personas Existentes\n\n```python\n# Para Dr. Gasnelio (t√©cnico)\ndef dr_gasnelio_response(question):\n    instruction = \"Como Dr. Gasnelio, responda tecnicamente:\"\n    return generate_hanseniase_response(instruction, question, \"dr_gasnelio\")\n\n# Para G√° (emp√°tico)\ndef ga_response(question):\n    instruction = \"Como G√°, responda de forma simples e emp√°tica:\"\n    return generate_hanseniase_response(instruction, question, \"ga_empathetic\")\n```\n\n### 5. Cache e Otimiza√ß√µes\n\n```python\n# Implementar cache para respostas frequentes\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_hanseniase_response(instruction, input_text, persona):\n    return generate_hanseniase_response(instruction, input_text, persona)\n```\n\n## üìä M√©tricas de Performance\n\n- **Perplexity:** [Adicionar ap√≥s avalia√ß√£o]\n- **BLEU Score:** [Adicionar ap√≥s avalia√ß√£o]\n- **Tempo de Infer√™ncia:** ~100-200ms por resposta\n- **Mem√≥ria GPU:** ~2-4GB para infer√™ncia\n\n## üîß Troubleshooting\n\n### Problema: OutOfMemoryError\n**Solu√ß√£o:** \n- Reduzir max_length para 256\n- Usar quantiza√ß√£o int8\n- Processar em batches menores\n\n### Problema: Respostas inconsistentes\n**Solu√ß√£o:**\n- Ajustar temperature (0.3-0.8)\n- Verificar formata√ß√£o do prompt\n- Adicionar mais exemplos de treinamento\n\n### Problema: Lat√™ncia alta\n**Solu√ß√£o:**\n- Fazer merge dos weights LoRA\n- Usar TensorRT ou ONNX\n- Implementar batch processing\n\n## üìà Pr√≥ximos Passos\n\n1. **Avaliar performance** em dados reais\n2. **Coletar feedback** dos usu√°rios\n3. **Retreinar** com novos exemplos\n4. **Otimizar** para produ√ß√£o\n5. **Implementar** A/B testing\n\n---\n\n**Gerado em:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n**Vers√£o:** Q2-2025-ML-MODERNIZATION\n\"\"\"\n\n# Salvar guia de integra√ß√£o\nintegration_file = f'{DRIVE_PATH}/integration_guide.md'\nwith open(integration_file, 'w', encoding='utf-8') as f:\n    f.write(integration_guide)\n\nprint(\"üìñ Guia de integra√ß√£o gerado:\")\nprint(f\"üìÅ Arquivo: {integration_file}\")\nprint(\"\\n\" + \"=\"*60)\nprint(integration_guide[:1000] + \"...\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üéâ 10. Resumo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Resumo final do treinamento\nprint(\"üéâ FINE-TUNING CONCLU√çDO COM SUCESSO!\")\nprint(\"=\"*60)\n\nprint(\"\\nüìä ESTAT√çSTICAS FINAIS:\")\nprint(f\"  üß† Modelo: {MODEL_NAME}\")\nprint(f\"  üìà Exemplos de treino: {len(tokenized_datasets['train'])}\")\nprint(f\"  üîç Exemplos de valida√ß√£o: {len(tokenized_datasets['validation'])}\")\nprint(f\"  üß™ Exemplos de teste: {len(tokenized_datasets['test'])}\")\nprint(f\"  ‚öôÔ∏è Par√¢metros trein√°veis: {trainable_params:,} ({percentage:.2f}%)\")\nprint(f\"  üîÑ Epochs: {training_args.num_train_epochs}\")\nprint(f\"  üìö LoRA Rank: {lora_config.r}\")\n\nprint(\"\\nüé≠ PERSONAS TREINADAS:\")\nfor persona, percentage in training_data['statistics']['persona_distribution'].items():\n    print(f\"  ‚Ä¢ {persona}: {percentage:.1f}%\")\n\nprint(\"\\nüìÅ ARQUIVOS GERADOS:\")\nprint(f\"  ‚Ä¢ Modelo: {training_args.output_dir}\")\nprint(f\"  ‚Ä¢ Backup no Drive: {DRIVE_PATH}/hanseniase_fine_tuned_model\")\nprint(f\"  ‚Ä¢ Guia de integra√ß√£o: {DRIVE_PATH}/integration_guide.md\")\nprint(f\"  ‚Ä¢ M√©tricas: {DRIVE_PATH}/hanseniase_fine_tuned_model/training_metrics.json\")\n\nprint(\"\\nüöÄ PR√ìXIMOS PASSOS:\")\nprint(\"  1. ‚úÖ Fazer download dos arquivos do modelo\")\nprint(\"  2. ‚úÖ Integrar no backend Python\")\nprint(\"  3. ‚úÖ Testar com dados reais\")\nprint(\"  4. ‚úÖ Coletar feedback dos usu√°rios\")\nprint(\"  5. ‚úÖ Monitorar performance em produ√ß√£o\")\n\nprint(\"\\nüí° DICAS DE USO:\")\nprint(\"  ‚Ä¢ Use temperature=0.7 para respostas balanceadas\")\nprint(\"  ‚Ä¢ Implemente cache para queries frequentes\")\nprint(\"  ‚Ä¢ Monitore lat√™ncia em produ√ß√£o\")\nprint(\"  ‚Ä¢ Colete feedback para retraining futuro\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üè• MODELO PRONTO PARA DISPENSA√á√ÉO DE HANSEN√çASE! üè•\")\nprint(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}