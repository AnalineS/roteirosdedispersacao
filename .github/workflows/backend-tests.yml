name: Backend Test Suite

on:
  push:
    branches: [ main, hml, develop ]
    paths:
      - 'apps/backend/**'
      - '.github/workflows/backend-tests.yml'
  pull_request:
    branches: [ main, hml ]
    paths:
      - 'apps/backend/**'
      - '.github/workflows/backend-tests.yml'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - performance
        - security

env:
  # Test environment configuration
  TESTING: true
  ENVIRONMENT: testing
  SECRET_KEY: test-secret-key-for-ci-only
  RATE_LIMIT_ENABLED: false
  CACHE_ENABLED: false
  EMBEDDINGS_ENABLED: false
  RAG_AVAILABLE: false
  QA_ENABLED: false
  EMAIL_ENABLED: false
  METRICS_ENABLED: false

  # Mock API keys for testing
  OPENROUTER_API_KEY: test-key-ci
  HUGGINGFACE_API_KEY: test-key-ci

  # Database settings for testing
  SQLITE_DB_PATH: ':memory:'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      test-type: ${{ steps.set-test-type.outputs.test-type }}
    steps:
      - name: Set test type
        id: set-test-type
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "test-type=${{ github.event.inputs.test_type }}" >> $GITHUB_OUTPUT
          else
            echo "test-type=all" >> $GITHUB_OUTPUT
          fi

      - name: Set test matrix
        id: set-matrix
        run: |
          if [ "${{ steps.set-test-type.outputs.test-type }}" = "all" ]; then
            echo 'matrix={"test-suite": ["unit", "integration", "performance", "security", "system"]}' >> $GITHUB_OUTPUT
          else
            echo 'matrix={"test-suite": ["${{ steps.set-test-type.outputs.test-type }}"]}' >> $GITHUB_OUTPUT
          fi

  backend-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          tesseract-ocr \
          tesseract-ocr-por \
          tesseract-ocr-eng \
          libgl1-mesa-glx \
          libglib2.0-0 \
          libsm6 \
          libxext6 \
          libxrender-dev \
          libgomp1 \
          libgtk-3-0 \
          sqlite3

    - name: Install Python dependencies
      working-directory: ./apps/backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-html pytest-json-report

    - name: Create test directories
      working-directory: ./apps/backend
      run: |
        mkdir -p logs
        mkdir -p cache
        mkdir -p htmlcov
        mkdir -p test-results

    - name: Run Core Functionality Tests
      if: matrix.test-suite == 'unit' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_00_core_functionality.py \
          -v \
          --tb=short \
          --maxfail=5 \
          --html=test-results/core-functionality-report.html \
          --json-report --json-report-file=test-results/core-functionality.json \
          -m "not slow"

    - name: Run Blueprint Functionality Tests
      if: matrix.test-suite == 'unit' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_01_blueprint_functionality.py \
          -v \
          --tb=short \
          --maxfail=10 \
          --html=test-results/blueprint-functionality-report.html \
          --json-report --json-report-file=test-results/blueprint-functionality.json \
          -m "not slow"

    - name: Run Integration Tests
      if: matrix.test-suite == 'integration' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_02_integration_workflows.py \
          -v \
          --tb=short \
          --maxfail=5 \
          --html=test-results/integration-report.html \
          --json-report --json-report-file=test-results/integration.json \
          -m "integration and not slow"

    - name: Run Performance Tests
      if: matrix.test-suite == 'performance' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_03_performance_load.py \
          -v \
          --tb=short \
          --maxfail=3 \
          --html=test-results/performance-report.html \
          --json-report --json-report-file=test-results/performance.json \
          -m "performance and not slow" \
          --timeout=300

    - name: Run Security Tests
      if: matrix.test-suite == 'security' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_04_security_validation.py \
          -v \
          --tb=short \
          --maxfail=5 \
          --html=test-results/security-report.html \
          --json-report --json-report-file=test-results/security.json \
          -m "security and not slow"

    - name: Run System Validation Tests
      if: matrix.test-suite == 'system' || matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_05_system_validation.py \
          -v \
          --tb=short \
          --maxfail=3 \
          --html=test-results/system-validation-report.html \
          --json-report --json-report-file=test-results/system-validation.json \
          -m "integration and not slow"

    - name: Run Coverage Analysis
      if: matrix.test-suite == 'all'
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/ \
          --cov=services \
          --cov=blueprints \
          --cov=core \
          --cov-branch \
          --cov-report=html:htmlcov \
          --cov-report=xml:coverage.xml \
          --cov-report=term-missing \
          --cov-fail-under=70 \
          -m "not slow and not performance"

    - name: Generate Test Summary
      if: always()
      working-directory: ./apps/backend
      run: |
        echo "## Test Results Summary" >> test_summary.md
        echo "" >> test_summary.md
        echo "**Test Suite**: ${{ matrix.test-suite }}" >> test_summary.md
        echo "**Branch**: ${{ github.ref_name }}" >> test_summary.md
        echo "**Commit**: ${{ github.sha }}" >> test_summary.md
        echo "" >> test_summary.md

        # Count test results
        if ls test-results/*.json 1> /dev/null 2>&1; then
          echo "### Test Counts" >> test_summary.md
          python -c "
        import json
        import glob

        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_skipped = 0

        for file in glob.glob('test-results/*.json'):
            try:
                with open(file) as f:
                    data = json.load(f)
                    summary = data.get('summary', {})
                    total_tests += summary.get('total', 0)
                    total_passed += summary.get('passed', 0)
                    total_failed += summary.get('failed', 0)
                    total_skipped += summary.get('skipped', 0)
            except:
                pass

        print(f'- Total Tests: {total_tests}')
        print(f'- Passed: {total_passed}')
        print(f'- Failed: {total_failed}')
        print(f'- Skipped: {total_skipped}')
        print(f'- Success Rate: {(total_passed/total_tests*100) if total_tests > 0 else 0:.1f}%')
        " >> test_summary.md
        fi

    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.test-suite }}
        path: |
          apps/backend/test-results/
          apps/backend/htmlcov/
          apps/backend/coverage.xml
          apps/backend/test_summary.md
        retention-days: 30

    - name: Upload Coverage to Codecov
      if: matrix.test-suite == 'all' && github.event_name != 'workflow_dispatch'
      uses: codecov/codecov-action@v3
      with:
        file: ./apps/backend/coverage.xml
        flags: backend
        name: backend-coverage
        fail_ci_if_error: false

  quality-gates:
    needs: [setup, backend-tests]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Download Test Results
      uses: actions/download-artifact@v3
      with:
        path: test-artifacts

    - name: Evaluate Quality Gates
      run: |
        echo "## Quality Gates Evaluation" > quality_report.md
        echo "" >> quality_report.md

        # Check if any test suite failed
        failed_suites=""
        success_rate=0
        total_tests=0

        for artifact_dir in test-artifacts/test-results-*/; do
          if [ -d "$artifact_dir" ]; then
            suite_name=$(basename "$artifact_dir" | sed 's/test-results-//')
            echo "Checking suite: $suite_name"

            # Look for JSON test results
            for json_file in "$artifact_dir"test-results/*.json; do
              if [ -f "$json_file" ]; then
                # Extract test results using basic tools
                if grep -q '"failed"' "$json_file"; then
                  failed_count=$(grep -o '"failed":[0-9]*' "$json_file" | grep -o '[0-9]*' || echo "0")
                  passed_count=$(grep -o '"passed":[0-9]*' "$json_file" | grep -o '[0-9]*' || echo "0")
                  total_count=$(grep -o '"total":[0-9]*' "$json_file" | grep -o '[0-9]*' || echo "0")

                  if [ "$failed_count" -gt 0 ]; then
                    failed_suites="$failed_suites $suite_name"
                  fi

                  total_tests=$((total_tests + total_count))
                  if [ "$total_count" -gt 0 ]; then
                    suite_success=$((passed_count * 100 / total_count))
                    echo "- $suite_name: $suite_success% success rate" >> quality_report.md
                  fi
                fi
              fi
            done
          fi
        done

        echo "" >> quality_report.md
        echo "### Quality Gate Results" >> quality_report.md

        if [ -n "$failed_suites" ]; then
          echo "❌ **FAILED**: Test suites with failures:$failed_suites" >> quality_report.md
          echo "quality_gate=failed" >> $GITHUB_ENV
        else
          echo "✅ **PASSED**: All test suites passed" >> quality_report.md
          echo "quality_gate=passed" >> $GITHUB_ENV
        fi

    - name: Comment Quality Report on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const quality_report = fs.readFileSync('quality_report.md', 'utf8');

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: quality_report
          });

    - name: Fail if Quality Gates Failed
      if: env.quality_gate == 'failed'
      run: |
        echo "Quality gates failed. Some test suites have failures."
        exit 1

  security-scan:
    runs-on: ubuntu-latest
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.test_type == 'security' || github.event.inputs.test_type == 'all'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install security tools
      run: |
        pip install safety bandit semgrep

    - name: Run Safety Check
      working-directory: ./apps/backend
      run: |
        safety check --json --output safety-report.json || true
        safety check || echo "Safety check completed with warnings"

    - name: Run Bandit Security Scan
      working-directory: ./apps/backend
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . || echo "Bandit scan completed with warnings"

    - name: Run Semgrep Scan
      working-directory: ./apps/backend
      run: |
        semgrep --config=auto --json --output=semgrep-report.json . || true
        semgrep --config=auto . || echo "Semgrep scan completed with warnings"

    - name: Upload Security Reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          apps/backend/safety-report.json
          apps/backend/bandit-report.json
          apps/backend/semgrep-report.json
        retention-days: 30

  performance-benchmark:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'all')
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      working-directory: ./apps/backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark

    - name: Run Performance Benchmarks
      working-directory: ./apps/backend
      run: |
        python -m pytest tests/test_03_performance_load.py \
          -v \
          --benchmark-json=benchmark-results.json \
          -m "performance and not slow" \
          --timeout=600

    - name: Store Benchmark Results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.ref == 'refs/heads/main'
      with:
        tool: 'pytest'
        output-file-path: apps/backend/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: false

  notify-results:
    needs: [backend-tests, quality-gates, security-scan]
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/hml')
    steps:
    - name: Notify Success
      if: needs.quality-gates.result == 'success'
      run: |
        echo "✅ All backend tests passed successfully!"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"

    - name: Notify Failure
      if: needs.quality-gates.result == 'failure'
      run: |
        echo "❌ Backend tests failed!"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        echo "Please check the test results and fix the issues."