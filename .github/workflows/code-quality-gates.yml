name: "üìã Code Quality Gates & Standards"

on:
  push:
    branches: [main, hml]
  pull_request:
    branches: [main, hml]
  schedule:
    # Run quality analysis weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      analysis_depth:
        description: 'Analysis depth'
        required: true
        default: 'standard'
        type: choice
        options:
          - 'quick'
          - 'standard'
          - 'comprehensive'
      fix_issues:
        description: 'Auto-fix issues where possible'
        type: boolean
        default: false
      create_report:
        description: 'Create detailed quality report'
        type: boolean
        default: true

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'

permissions:
  contents: write
  pull-requests: write
  actions: read
  checks: write

jobs:
  # ============================================================================
  # FRONTEND CODE QUALITY ANALYSIS
  # ============================================================================
  frontend-quality:
    name: "‚öõÔ∏è Frontend Code Quality"
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      eslint-score: ${{ steps.eslint-analysis.outputs.score }}
      typescript-score: ${{ steps.typescript-analysis.outputs.score }}
      quality-score: ${{ steps.quality-summary.outputs.score }}
    steps:
      - uses: actions/checkout@v4

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: apps/frontend-nextjs/package-lock.json

      - name: "üì¶ Install Dependencies"
        run: |
          cd apps/frontend-nextjs
          npm ci

      - name: "üîç ESLint Analysis"
        id: eslint-analysis
        run: |
          cd apps/frontend-nextjs
          echo "üîç Running ESLint analysis..."

          # Run ESLint with detailed output
          if npm run lint -- --format=json --output-file=/tmp/eslint-report.json; then
              ESLINT_EXIT_CODE=0
          else
              ESLINT_EXIT_CODE=1
          fi

          # Parse ESLint results
          if [[ -f "/tmp/eslint-report.json" ]]; then
              ERROR_COUNT=$(cat /tmp/eslint-report.json | jq '[.[].messages[] | select(.severity == 2)] | length' 2>/dev/null || echo "0")
              WARNING_COUNT=$(cat /tmp/eslint-report.json | jq '[.[].messages[] | select(.severity == 1)] | length' 2>/dev/null || echo "0")
              TOTAL_FILES=$(cat /tmp/eslint-report.json | jq '. | length' 2>/dev/null || echo "0")

              echo "üìä ESLint Results:"
              echo "  - Files analyzed: $TOTAL_FILES"
              echo "  - Errors: $ERROR_COUNT"
              echo "  - Warnings: $WARNING_COUNT"

              # Calculate ESLint score (100 - errors*2 - warnings*0.5, minimum 0)
              ESLINT_SCORE=$(echo "scale=0; 100 - ($ERROR_COUNT * 2) - ($WARNING_COUNT * 0.5)" | bc -l)
              if [[ $(echo "$ESLINT_SCORE < 0" | bc -l) == "1" ]]; then
                  ESLINT_SCORE=0
              fi

              echo "üéØ ESLint Score: $ESLINT_SCORE/100"
              echo "score=$ESLINT_SCORE" >> $GITHUB_OUTPUT

              # Extract top issues
              if [[ "$ERROR_COUNT" -gt "0" ]] || [[ "$WARNING_COUNT" -gt "0" ]]; then
                  echo "üö® Top ESLint Issues:"
                  cat /tmp/eslint-report.json | jq -r '.[].messages[] | "\(.ruleId): \(.message) (Line \(.line))"' | head -10
              fi
          else
              echo "‚ùå ESLint report not generated"
              echo "score=0" >> $GITHUB_OUTPUT
          fi

          # Auto-fix if requested and not in PR context
          if [[ "${{ github.event.inputs.fix_issues }}" == "true" && "${{ github.event_name }}" != "pull_request" ]]; then
              echo "üîß Auto-fixing ESLint issues..."
              npm run lint:fix || echo "Some issues could not be auto-fixed"
          fi

      - name: "üìù TypeScript Analysis"
        id: typescript-analysis
        run: |
          cd apps/frontend-nextjs
          echo "üìù Running TypeScript analysis..."

          # Type checking
          if npm run type-check 2>&1 | tee /tmp/typescript-output.txt; then
              TS_EXIT_CODE=0
              echo "‚úÖ TypeScript compilation successful"
              TS_SCORE=100
          else
              TS_EXIT_CODE=1
              echo "‚ùå TypeScript compilation errors detected"

              # Count TypeScript errors
              TS_ERRORS=$(grep -c "error TS" /tmp/typescript-output.txt 2>/dev/null || echo "0")
              echo "üìä TypeScript errors: $TS_ERRORS"

              # Calculate TypeScript score
              TS_SCORE=$(echo "scale=0; 100 - ($TS_ERRORS * 5)" | bc -l)
              if [[ $(echo "$TS_SCORE < 0" | bc -l) == "1" ]]; then
                  TS_SCORE=0
              fi

              echo "üö® TypeScript issues:"
              head -20 /tmp/typescript-output.txt
          fi

          echo "üéØ TypeScript Score: $TS_SCORE/100"
          echo "score=$TS_SCORE" >> $GITHUB_OUTPUT

      - name: "üß™ Frontend Testing Quality"
        run: |
          cd apps/frontend-nextjs
          echo "üß™ Analyzing test coverage and quality..."

          # Run tests with coverage
          if npm run test:coverage -- --passWithNoTests 2>&1 | tee /tmp/test-output.txt; then
              echo "‚úÖ Tests passed"

              # Extract coverage information if available
              if grep -q "Coverage summary" /tmp/test-output.txt; then
                  echo "üìä Test Coverage Summary:"
                  grep -A 10 "Coverage summary" /tmp/test-output.txt || echo "Coverage details not available"
              fi
          else
              echo "‚ö†Ô∏è Some tests failed or no tests found"
          fi

      - name: "üìä Frontend Quality Summary"
        id: quality-summary
        run: |
          cd apps/frontend-nextjs

          ESLINT_SCORE="${{ steps.eslint-analysis.outputs.score }}"
          TYPESCRIPT_SCORE="${{ steps.typescript-analysis.outputs.score }}"

          # Calculate overall frontend quality score
          OVERALL_SCORE=$(echo "scale=0; ($ESLINT_SCORE + $TYPESCRIPT_SCORE) / 2" | bc -l)

          echo "üìä Frontend Quality Summary:"
          echo "  - ESLint Score: $ESLINT_SCORE/100"
          echo "  - TypeScript Score: $TYPESCRIPT_SCORE/100"
          echo "  - Overall Score: $OVERALL_SCORE/100"

          echo "score=$OVERALL_SCORE" >> $GITHUB_OUTPUT

          # Quality gates
          if [[ "$OVERALL_SCORE" -lt "70" ]]; then
              echo "‚ùå Frontend quality gate FAILED (score < 70)"
              echo "::error::Frontend code quality below acceptable threshold: $OVERALL_SCORE/100"
              exit 1
          elif [[ "$OVERALL_SCORE" -lt "85" ]]; then
              echo "‚ö†Ô∏è Frontend quality needs improvement (score < 85)"
              echo "::warning::Frontend code quality could be improved: $OVERALL_SCORE/100"
          else
              echo "‚úÖ Frontend quality gate PASSED"
          fi

      - name: "üìÑ Store Frontend Quality Reports"
        uses: actions/upload-artifact@v4
        with:
          name: frontend-quality-reports
          path: |
            /tmp/eslint-report.json
            /tmp/typescript-output.txt
            /tmp/test-output.txt
          retention-days: 30

  # ============================================================================
  # BACKEND CODE QUALITY ANALYSIS
  # ============================================================================
  backend-quality:
    name: "üêç Backend Code Quality"
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      pylint-score: ${{ steps.pylint-analysis.outputs.score }}
      flake8-score: ${{ steps.flake8-analysis.outputs.score }}
      mypy-score: ${{ steps.mypy-analysis.outputs.score }}
      quality-score: ${{ steps.quality-summary.outputs.score }}
    steps:
      - uses: actions/checkout@v4

      - name: "üêç Setup Python"
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: "üì¶ Install Dependencies"
        run: |
          cd apps/backend
          pip install -r requirements.txt
          pip install pylint flake8 mypy black isort

      - name: "üîç Pylint Analysis"
        id: pylint-analysis
        run: |
          cd apps/backend
          echo "üîç Running Pylint analysis..."

          # Run pylint with scoring
          pylint --output-format=json --reports=y . > /tmp/pylint-report.json 2>/dev/null || true

          # Extract pylint score
          if [[ -f "/tmp/pylint-report.json" ]]; then
              # Try to extract score from pylint output
              PYLINT_SCORE=$(pylint --reports=y . 2>/dev/null | grep "Your code has been rated at" | grep -o "[0-9]*\.[0-9]*" | head -1 || echo "0")

              if [[ -z "$PYLINT_SCORE" ]] || [[ "$PYLINT_SCORE" == "0" ]]; then
                  # Fallback: calculate score based on issues
                  PYLINT_ISSUES=$(cat /tmp/pylint-report.json | jq '. | length' 2>/dev/null || echo "100")
                  PYLINT_SCORE=$(echo "scale=1; 100 - ($PYLINT_ISSUES * 0.5)" | bc -l)
                  if [[ $(echo "$PYLINT_SCORE < 0" | bc -l) == "1" ]]; then
                      PYLINT_SCORE=0
                  fi
              fi

              echo "üéØ Pylint Score: $PYLINT_SCORE/10"

              # Convert to 100-point scale
              PYLINT_SCORE_100=$(echo "scale=0; $PYLINT_SCORE * 10" | bc -l)
              echo "score=$PYLINT_SCORE_100" >> $GITHUB_OUTPUT

              # Show top issues
              if [[ -s "/tmp/pylint-report.json" ]]; then
                  echo "üö® Top Pylint Issues:"
                  cat /tmp/pylint-report.json | jq -r '.[] | "\(.type): \(.message) (\(.path):\(.line))"' | head -10
              fi
          else
              echo "‚ö†Ô∏è Pylint analysis failed"
              echo "score=50" >> $GITHUB_OUTPUT
          fi

      - name: "üîç Flake8 Analysis"
        id: flake8-analysis
        run: |
          cd apps/backend
          echo "üîç Running Flake8 analysis..."

          # Run flake8
          if flake8 --output-file=/tmp/flake8-report.txt --statistics .; then
              echo "‚úÖ Flake8 passed with no issues"
              FLAKE8_SCORE=100
          else
              echo "‚ö†Ô∏è Flake8 found style issues"
              FLAKE8_ISSUES=$(wc -l < /tmp/flake8-report.txt 2>/dev/null || echo "0")
              echo "üìä Flake8 issues: $FLAKE8_ISSUES"

              # Calculate flake8 score
              FLAKE8_SCORE=$(echo "scale=0; 100 - ($FLAKE8_ISSUES * 1)" | bc -l)
              if [[ $(echo "$FLAKE8_SCORE < 0" | bc -l) == "1" ]]; then
                  FLAKE8_SCORE=0
              fi

              echo "üö® Top Flake8 Issues:"
              head -10 /tmp/flake8-report.txt
          fi

          echo "üéØ Flake8 Score: $FLAKE8_SCORE/100"
          echo "score=$FLAKE8_SCORE" >> $GITHUB_OUTPUT

      - name: "üîç MyPy Type Analysis"
        id: mypy-analysis
        run: |
          cd apps/backend
          echo "üîç Running MyPy type analysis..."

          # Run mypy
          if mypy . --ignore-missing-imports --output=/tmp/mypy-report.txt 2>&1; then
              echo "‚úÖ MyPy type checking passed"
              MYPY_SCORE=100
          else
              echo "‚ö†Ô∏è MyPy found type issues"
              MYPY_ERRORS=$(grep -c "error:" /tmp/mypy-report.txt 2>/dev/null || echo "0")
              echo "üìä MyPy errors: $MYPY_ERRORS"

              # Calculate mypy score
              MYPY_SCORE=$(echo "scale=0; 100 - ($MYPY_ERRORS * 5)" | bc -l)
              if [[ $(echo "$MYPY_SCORE < 0" | bc -l) == "1" ]]; then
                  MYPY_SCORE=0
              fi

              echo "üö® Top MyPy Issues:"
              head -10 /tmp/mypy-report.txt
          fi

          echo "üéØ MyPy Score: $MYPY_SCORE/100"
          echo "score=$MYPY_SCORE" >> $GITHUB_OUTPUT

      - name: "üß™ Backend Testing Quality"
        run: |
          cd apps/backend
          echo "üß™ Analyzing backend test quality..."

          # Check if tests exist
          if find . -name "*test*.py" -o -name "test_*.py" | grep -q .; then
              echo "‚úÖ Test files found"

              # Run tests with coverage if pytest is available
              if command -v pytest >/dev/null 2>&1; then
                  echo "üß™ Running pytest with coverage..."
                  pytest --cov=. --cov-report=term-missing --tb=short > /tmp/pytest-output.txt 2>&1 || echo "Some tests failed"

                  if [[ -f "/tmp/pytest-output.txt" ]]; then
                      echo "üìä Test Results:"
                      grep -E "(TOTAL|test session starts|failed|passed)" /tmp/pytest-output.txt || head -20 /tmp/pytest-output.txt
                  fi
              else
                  echo "‚ö†Ô∏è pytest not available, skipping test coverage analysis"
              fi
          else
              echo "‚ö†Ô∏è No test files found in backend"
          fi

      - name: "üîß Code Formatting Check"
        run: |
          cd apps/backend
          echo "üîß Checking code formatting..."

          # Check Black formatting
          if black --check --diff . > /tmp/black-output.txt 2>&1; then
              echo "‚úÖ Code is properly formatted with Black"
          else
              echo "‚ö†Ô∏è Code formatting issues found"
              echo "üîß Black formatting suggestions:"
              head -20 /tmp/black-output.txt

              # Auto-fix if requested
              if [[ "${{ github.event.inputs.fix_issues }}" == "true" && "${{ github.event_name }}" != "pull_request" ]]; then
                  echo "üîß Auto-formatting with Black..."
                  black .
              fi
          fi

          # Check import sorting
          if isort --check-only --diff . > /tmp/isort-output.txt 2>&1; then
              echo "‚úÖ Imports are properly sorted"
          else
              echo "‚ö†Ô∏è Import sorting issues found"
              echo "üîß Import sorting suggestions:"
              head -10 /tmp/isort-output.txt

              # Auto-fix if requested
              if [[ "${{ github.event.inputs.fix_issues }}" == "true" && "${{ github.event_name }}" != "pull_request" ]]; then
                  echo "üîß Auto-sorting imports..."
                  isort .
              fi
          fi

      - name: "üìä Backend Quality Summary"
        id: quality-summary
        run: |
          PYLINT_SCORE="${{ steps.pylint-analysis.outputs.score }}"
          FLAKE8_SCORE="${{ steps.flake8-analysis.outputs.score }}"
          MYPY_SCORE="${{ steps.mypy-analysis.outputs.score }}"

          # Calculate overall backend quality score
          OVERALL_SCORE=$(echo "scale=0; ($PYLINT_SCORE + $FLAKE8_SCORE + $MYPY_SCORE) / 3" | bc -l)

          echo "üìä Backend Quality Summary:"
          echo "  - Pylint Score: $PYLINT_SCORE/100"
          echo "  - Flake8 Score: $FLAKE8_SCORE/100"
          echo "  - MyPy Score: $MYPY_SCORE/100"
          echo "  - Overall Score: $OVERALL_SCORE/100"

          echo "score=$OVERALL_SCORE" >> $GITHUB_OUTPUT

          # Quality gates
          if [[ "$OVERALL_SCORE" -lt "70" ]]; then
              echo "‚ùå Backend quality gate FAILED (score < 70)"
              echo "::error::Backend code quality below acceptable threshold: $OVERALL_SCORE/100"
              exit 1
          elif [[ "$OVERALL_SCORE" -lt "85" ]]; then
              echo "‚ö†Ô∏è Backend quality needs improvement (score < 85)"
              echo "::warning::Backend code quality could be improved: $OVERALL_SCORE/100"
          else
              echo "‚úÖ Backend quality gate PASSED"
          fi

      - name: "üìÑ Store Backend Quality Reports"
        uses: actions/upload-artifact@v4
        with:
          name: backend-quality-reports
          path: |
            /tmp/pylint-report.json
            /tmp/flake8-report.txt
            /tmp/mypy-report.txt
            /tmp/pytest-output.txt
            /tmp/black-output.txt
            /tmp/isort-output.txt
          retention-days: 30

  # ============================================================================
  # ARCHITECTURE & DESIGN QUALITY
  # ============================================================================
  architecture-quality:
    name: "üèóÔ∏è Architecture Quality"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      architecture-score: ${{ steps.architecture-analysis.outputs.score }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "üèóÔ∏è Architecture Analysis"
        id: architecture-analysis
        run: |
          echo "üèóÔ∏è Analyzing project architecture quality..."

          ARCHITECTURE_SCORE=100
          ISSUES=()

          # Check project structure
          echo "üìÅ Checking project structure..."

          # Backend structure validation
          REQUIRED_BACKEND_DIRS=("services" "blueprints" "core" "core/security")
          for dir in "${REQUIRED_BACKEND_DIRS[@]}"; do
              if [[ ! -d "apps/backend/$dir" ]]; then
                  ISSUES+=("Missing backend directory: $dir")
                  ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 10))
              fi
          done

          # Frontend structure validation
          REQUIRED_FRONTEND_DIRS=("src/components" "src/hooks" "src/services" "src/types")
          for dir in "${REQUIRED_FRONTEND_DIRS[@]}"; do
              if [[ ! -d "apps/frontend-nextjs/$dir" ]]; then
                  ISSUES+=("Missing frontend directory: $dir")
                  ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 10))
              fi
          done

          # Check for architectural patterns
          echo "üîç Checking architectural patterns..."

          # Blueprint count (should be <= 8 for simplified architecture)
          BLUEPRINT_COUNT=$(find apps/backend/blueprints -name "*_blueprint.py" 2>/dev/null | wc -l)
          echo "üì¶ Blueprint count: $BLUEPRINT_COUNT"

          if [[ $BLUEPRINT_COUNT -gt 8 ]]; then
              ISSUES+=("Too many blueprints ($BLUEPRINT_COUNT > 8) - violates simplified architecture")
              ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 15))
          fi

          # Check for circular dependencies (simplified check)
          echo "üîÑ Checking for potential circular dependencies..."

          # Python circular dependency check
          if find apps/backend -name "*.py" -exec grep -l "from.*import.*\*" {} \; | head -5; then
              ISSUES+=("Wildcard imports detected - potential circular dependency risk")
              ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 5))
          fi

          # Check documentation
          echo "üìö Checking documentation quality..."

          DOC_SCORE=0
          if [[ -f "README.md" ]]; then DOC_SCORE=$((DOC_SCORE + 25)); fi
          if [[ -f "CLAUDE.md" ]]; then DOC_SCORE=$((DOC_SCORE + 25)); fi
          if find docs -name "*.md" 2>/dev/null | grep -q .; then DOC_SCORE=$((DOC_SCORE + 25)); fi
          if find apps -name "*.md" 2>/dev/null | grep -q .; then DOC_SCORE=$((DOC_SCORE + 25)); fi

          echo "üìä Documentation score: $DOC_SCORE/100"
          ARCHITECTURE_SCORE=$(echo "scale=0; $ARCHITECTURE_SCORE + ($DOC_SCORE * 0.2)" | bc -l)

          # Check code organization
          echo "üóÇÔ∏è Checking code organization..."

          # Look for large files (potential code smell)
          LARGE_FILES=$(find apps -name "*.py" -o -name "*.ts" -o -name "*.tsx" | xargs wc -l | sort -nr | head -5 | awk '$1 > 500 {print $2 " (" $1 " lines)"}')
          if [[ -n "$LARGE_FILES" ]]; then
              echo "‚ö†Ô∏è Large files detected (potential refactoring candidates):"
              echo "$LARGE_FILES"
              ISSUES+=("Large files detected - consider refactoring")
              ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 5))
          fi

          # Check for magic numbers and hardcoded values
          MAGIC_NUMBERS=$(find apps -name "*.py" -o -name "*.ts" -o -name "*.tsx" | xargs grep -n "[^a-zA-Z0-9_]\(25[0-5]\|2[0-4][0-9]\|1[0-9][0-9]\|[1-9][0-9]\|[1-9]\)[^a-zA-Z0-9_]" | head -10)
          if [[ -n "$MAGIC_NUMBERS" ]]; then
              echo "‚ö†Ô∏è Potential magic numbers detected"
              ISSUES+=("Magic numbers detected - consider using constants")
              ARCHITECTURE_SCORE=$((ARCHITECTURE_SCORE - 5))
          fi

          # Ensure minimum score
          if [[ $ARCHITECTURE_SCORE -lt 0 ]]; then
              ARCHITECTURE_SCORE=0
          fi

          echo "üìä Architecture Quality Score: $ARCHITECTURE_SCORE/100"
          echo "score=$ARCHITECTURE_SCORE" >> $GITHUB_OUTPUT

          # Report issues
          if [[ ${#ISSUES[@]} -gt 0 ]]; then
              echo "üö® Architecture Issues Found:"
              printf '  - %s\n' "${ISSUES[@]}"
          else
              echo "‚úÖ No major architecture issues detected"
          fi

  # ============================================================================
  # COMPREHENSIVE QUALITY REPORT
  # ============================================================================
  quality-report:
    name: "üìä Quality Report Generation"
    runs-on: ubuntu-latest
    needs: [frontend-quality, backend-quality, architecture-quality]
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: "üìä Generate Comprehensive Quality Report"
        run: |
          echo "## üìã Code Quality Analysis Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Analysis Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Quality scores
          FRONTEND_SCORE="${{ needs.frontend-quality.outputs.quality-score }}"
          BACKEND_SCORE="${{ needs.backend-quality.outputs.quality-score }}"
          ARCHITECTURE_SCORE="${{ needs.architecture-quality.outputs.architecture-score }}"

          # Calculate overall quality score
          if [[ -n "$FRONTEND_SCORE" && -n "$BACKEND_SCORE" && -n "$ARCHITECTURE_SCORE" ]]; then
              OVERALL_SCORE=$(echo "scale=0; ($FRONTEND_SCORE + $BACKEND_SCORE + $ARCHITECTURE_SCORE) / 3" | bc -l)
          else
              OVERALL_SCORE="N/A"
          fi

          echo "### üìä Quality Scores" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Score | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|---------|" >> $GITHUB_STEP_SUMMARY

          # Frontend status
          if [[ "${{ needs.frontend-quality.result }}" == "success" ]]; then
              FRONTEND_STATUS="‚úÖ Pass"
          else
              FRONTEND_STATUS="‚ùå Fail"
          fi

          # Backend status
          if [[ "${{ needs.backend-quality.result }}" == "success" ]]; then
              BACKEND_STATUS="‚úÖ Pass"
          else
              BACKEND_STATUS="‚ùå Fail"
          fi

          # Architecture status
          if [[ "${{ needs.architecture-quality.result }}" == "success" ]]; then
              ARCHITECTURE_STATUS="‚úÖ Pass"
          else
              ARCHITECTURE_STATUS="‚ùå Fail"
          fi

          echo "| Frontend | $FRONTEND_SCORE/100 | $FRONTEND_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Backend | $BACKEND_SCORE/100 | $BACKEND_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Architecture | $ARCHITECTURE_SCORE/100 | $ARCHITECTURE_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| **Overall** | **$OVERALL_SCORE/100** | **Overall Assessment** |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Detailed breakdown
          echo "### üîç Detailed Analysis" >> $GITHUB_STEP_SUMMARY
          echo "#### ‚öõÔ∏è Frontend Quality" >> $GITHUB_STEP_SUMMARY
          echo "- ESLint Score: ${{ needs.frontend-quality.outputs.eslint-score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "- TypeScript Score: ${{ needs.frontend-quality.outputs.typescript-score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### üêç Backend Quality" >> $GITHUB_STEP_SUMMARY
          echo "- Pylint Score: ${{ needs.backend-quality.outputs.pylint-score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "- Flake8 Score: ${{ needs.backend-quality.outputs.flake8-score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "- MyPy Score: ${{ needs.backend-quality.outputs.mypy-score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Quality recommendations
          echo "### üí° Quality Recommendations" >> $GITHUB_STEP_SUMMARY
          if [[ "$OVERALL_SCORE" != "N/A" ]]; then
              if [[ "$OVERALL_SCORE" -ge "90" ]]; then
                  echo "üü¢ **Excellent**: Code quality is exceptional" >> $GITHUB_STEP_SUMMARY
              elif [[ "$OVERALL_SCORE" -ge "80" ]]; then
                  echo "üü° **Good**: Code quality is solid with room for minor improvements" >> $GITHUB_STEP_SUMMARY
              elif [[ "$OVERALL_SCORE" -ge "70" ]]; then
                  echo "üü† **Acceptable**: Code quality meets minimum standards but needs improvement" >> $GITHUB_STEP_SUMMARY
              else
                  echo "üî¥ **Needs Improvement**: Code quality is below standards and requires attention" >> $GITHUB_STEP_SUMMARY
              fi
          fi

          # Action items
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìã Action Items" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.frontend-quality.result }}" != "success" ]]; then
              echo "- üîß Address frontend code quality issues" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "${{ needs.backend-quality.result }}" != "success" ]]; then
              echo "- üîß Address backend code quality issues" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "${{ needs.architecture-quality.result }}" != "success" ]]; then
              echo "- üèóÔ∏è Review and improve architecture patterns" >> $GITHUB_STEP_SUMMARY
          fi
          echo "- üìä Review detailed reports in workflow artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- üîÑ Next quality check: Weekly or on code changes" >> $GITHUB_STEP_SUMMARY

      - name: "üíæ Save Quality Metrics"
        if: github.event.inputs.create_report == 'true'
        run: |
          # Create quality metrics file
          cat > quality-metrics.json << EOF
          {
            "timestamp": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
            "trigger": "${{ github.event_name }}",
            "scores": {
              "frontend": ${{ needs.frontend-quality.outputs.quality-score }},
              "backend": ${{ needs.backend-quality.outputs.quality-score }},
              "architecture": ${{ needs.architecture-quality.outputs.architecture-score }},
              "overall": $(echo "scale=0; (${{ needs.frontend-quality.outputs.quality-score }} + ${{ needs.backend-quality.outputs.quality-score }} + ${{ needs.architecture-quality.outputs.architecture-score }}) / 3" | bc -l)
            },
            "details": {
              "eslint": ${{ needs.frontend-quality.outputs.eslint-score }},
              "typescript": ${{ needs.frontend-quality.outputs.typescript-score }},
              "pylint": ${{ needs.backend-quality.outputs.pylint-score }},
              "flake8": ${{ needs.backend-quality.outputs.flake8-score }},
              "mypy": ${{ needs.backend-quality.outputs.mypy-score }}
            }
          }
          EOF

          echo "üíæ Quality metrics saved"

      - name: "üìÑ Store Quality Metrics"
        uses: actions/upload-artifact@v4
        with:
          name: quality-metrics
          path: quality-metrics.json
          retention-days: 90

      - name: "üö® Quality Alert"
        if: needs.frontend-quality.result != 'success' || needs.backend-quality.result != 'success'
        env:
          TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          if [[ -n "$TELEGRAM_TOKEN" && -n "$TELEGRAM_CHAT_ID" ]]; then
              echo "üö® Sending quality alert..."

              MESSAGE="üî¥ <b>Code Quality Alert</b>%0A"
              MESSAGE="${MESSAGE}%0Aüìä <b>Frontend:</b> ${{ needs.frontend-quality.outputs.quality-score }}/100"
              MESSAGE="${MESSAGE}%0Aüìä <b>Backend:</b> ${{ needs.backend-quality.outputs.quality-score }}/100"
              MESSAGE="${MESSAGE}%0Aüìä <b>Architecture:</b> ${{ needs.architecture-quality.outputs.architecture-score }}/100"
              MESSAGE="${MESSAGE}%0A‚è∞ <b>Time:</b> $(date '+%H:%M - %d/%m/%Y')"
              MESSAGE="${MESSAGE}%0A%0Aüîó <b>Report:</b> <a href=\"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\">View Details</a>"

              curl -s -X POST "https://api.telegram.org/bot$TELEGRAM_TOKEN/sendMessage" \
                -d chat_id="$TELEGRAM_CHAT_ID" \
                -d text="$MESSAGE" \
                -d parse_mode="HTML" \
                -d disable_web_page_preview="true" >/dev/null

              echo "‚úÖ Quality alert sent"
          else
              echo "‚ö†Ô∏è Telegram alerts not configured"
          fi