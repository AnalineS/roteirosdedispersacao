#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Migra√ß√£o JSON para Supabase - FASE 3 RAG
Migra 9 arquivos JSON estruturados para PostgreSQL + pgvector
Processa chunks m√©dicos e gera embeddings automaticamente
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
import hashlib

# Configurar encoding para Windows
if os.name == 'nt':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# Adicionar diret√≥rio parent ao path para imports
sys.path.append(str(Path(__file__).parent.parent))

from app_config import config
from services.medical_chunking import MedicalChunker, ChunkPriority
from services.supabase_vector_store import SupabaseVectorStore, VectorDocument
from services.embedding_service import get_embedding_service

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class JSONToSupabaseMigrator:
    """
    Migrador de dados JSON estruturados para Supabase
    Processa conhecimento m√©dico e gera embeddings
    """
    
    def __init__(self):
        self.config = config
        self.vector_store = SupabaseVectorStore(config)
        self.chunker = MedicalChunker()
        self.embedding_service = get_embedding_service()
        
        # Diret√≥rios de dados - corrigido para estrutura real
        # Backend est√° em apps/backend, ent√£o subir 3 n√≠veis para chegar na raiz
        self.data_dir = Path(__file__).parent.parent.parent.parent / "data" / "structured"
        
        # Mapeamento de arquivos JSON para categorias
        self.json_files = {
            "clinical_taxonomy.json": {
                "chunk_type": "taxonomy",
                "priority": 0.8,
                "description": "Taxonomia cl√≠nica m√©dica"
            },
            "dosing_protocols.json": {
                "chunk_type": "dosage", 
                "priority": 1.0,
                "description": "Protocolos de dosagem - CR√çTICO"
            },
            "medications_mechanisms.json": {
                "chunk_type": "mechanism",
                "priority": 0.7,
                "description": "Mecanismos de a√ß√£o dos medicamentos"
            },
            "dispensing_workflow.json": {
                "chunk_type": "protocol",
                "priority": 0.9,
                "description": "Fluxos de dispensa√ß√£o"
            },
            "pharmacovigilance_guidelines.json": {
                "chunk_type": "safety",
                "priority": 0.95,
                "description": "Diretrizes de farmacovigil√¢ncia"
            },
            "hanseniase_catalog.json": {
                "chunk_type": "catalog",
                "priority": 0.8,
                "description": "Cat√°logo hansen√≠ase"
            },
            "quick_reference_protocols.json": {
                "chunk_type": "reference",
                "priority": 0.85,
                "description": "Protocolos de refer√™ncia r√°pida"
            },
            "frequently_asked_questions.json": {
                "chunk_type": "faq",
                "priority": 0.6,
                "description": "Perguntas frequentes"
            },
            "knowledge_scope_limitations.json": {
                "chunk_type": "scope",
                "priority": 0.5,
                "description": "Limita√ß√µes de escopo"
            }
        }
        
        # Estat√≠sticas da migra√ß√£o
        self.migration_stats = {
            "files_processed": 0,
            "chunks_created": 0,
            "embeddings_generated": 0,
            "documents_indexed": 0,
            "errors": [],
            "start_time": datetime.now(),
            "end_time": None
        }
        
        logger.info(f"[START] JSONToSupabaseMigrator inicializado")
        logger.info(f"üìÅ Data directory: {self.data_dir}")
        logger.info(f"[LIST] Files to migrate: {len(self.json_files)}")
    
    def validate_environment(self) -> bool:
        """Valida se ambiente est√° configurado para migra√ß√£o"""
        errors = []
        
        # Verificar Supabase config
        if not self.config.SUPABASE_URL:
            errors.append("SUPABASE_URL n√£o configurado")
        
        if not self.config.SUPABASE_KEY:
            errors.append("SUPABASE_KEY n√£o configurado")
        
        # Verificar diret√≥rio de dados
        if not self.data_dir.exists():
            errors.append(f"Diret√≥rio de dados n√£o encontrado: {self.data_dir}")
        
        # Verificar arquivos JSON
        missing_files = []
        for filename in self.json_files.keys():
            file_path = self.data_dir / filename
            if not file_path.exists():
                missing_files.append(filename)
        
        if missing_files:
            errors.append(f"Arquivos JSON n√£o encontrados: {missing_files}")
        
        # Verificar vector store
        if not self.vector_store.client:
            errors.append("Conex√£o Supabase n√£o estabelecida")
        
        # Verificar servi√ßo de embeddings
        if self.embedding_service:
            if self.embedding_service.is_available():
                logger.info("[OK] Servi√ßo de embeddings dispon√≠vel")
            else:
                logger.warning("[WARNING] Servi√ßo de embeddings n√£o dispon√≠vel - documentos ser√£o criados sem embeddings")
        else:
            logger.warning("[WARNING] Servi√ßo de embeddings n√£o inicializado")
        
        if errors:
            logger.error("[ERROR] Valida√ß√£o de ambiente falhou:")
            for error in errors:
                logger.error(f"   - {error}")
            return False
        
        logger.info("[OK] Ambiente validado com sucesso")
        return True
    
    def load_json_file(self, filename: str) -> Optional[Dict]:
        """Carrega arquivo JSON com tratamento de erros"""
        file_path = self.data_dir / filename
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"[OK] JSON carregado: {filename} ({len(str(data))} chars)")
            return data
            
        except Exception as e:
            error_msg = f"Erro ao carregar {filename}: {e}"
            logger.error(error_msg)
            self.migration_stats["errors"].append(error_msg)
            return None
    
    def extract_chunks_from_json(self, data: Dict, file_info: Dict) -> List[Dict]:
        """Extrai chunks textuais de dados JSON estruturados"""
        chunks = []
        
        def extract_text_recursively(obj, path="", parent_key=""):
            """Extrai texto recursivamente de estrutura JSON"""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    extract_text_recursively(value, current_path, key)
                    
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    current_path = f"{path}[{i}]"
                    extract_text_recursively(item, current_path, parent_key)
                    
            elif isinstance(obj, str) and len(obj.strip()) > 20:
                # Texto significativo - criar chunk
                chunks.append({
                    "text": obj.strip(),
                    "path": path,
                    "parent_key": parent_key,
                    "chunk_type": file_info["chunk_type"],
                    "priority": file_info["priority"]
                })
        
        extract_text_recursively(data)
        
        # Processar chunks com chunker m√©dico para otimizar
        processed_chunks = []
        for chunk_data in chunks:
            if len(chunk_data["text"]) > 100:  # Apenas chunks substanciais
                
                # Determinar categoria espec√≠fica baseada em conte√∫do
                chunk_category = self._classify_chunk_content(
                    chunk_data["text"], 
                    chunk_data["chunk_type"]
                )
                
                # Criar chunk m√©dico
                medical_chunk = {
                    "text": chunk_data["text"],
                    "chunk_type": chunk_category,
                    "priority": self._adjust_priority_by_content(
                        chunk_data["text"], 
                        chunk_data["priority"]
                    ),
                    "metadata": {
                        "source_path": chunk_data["path"],
                        "parent_key": chunk_data["parent_key"],
                        "original_category": chunk_data["chunk_type"],
                        "word_count": len(chunk_data["text"].split())
                    }
                }
                
                processed_chunks.append(medical_chunk)
        
        logger.info(f"[NOTE] Extra√≠dos {len(processed_chunks)} chunks de texto significativos")
        return processed_chunks
    
    def _classify_chunk_content(self, text: str, base_category: str) -> str:
        """Classifica conte√∫do do chunk para categoria mais espec√≠fica"""
        text_lower = text.lower()
        
        # Palavras-chave para categoriza√ß√£o espec√≠fica
        dosage_keywords = ["dose", "dosagem", "mg", "comprimido", "administrar", "posologia"]
        safety_keywords = ["contraindica√ß√£o", "efeito colateral", "rea√ß√£o adversa", "cuidado", "aten√ß√£o"]
        protocol_keywords = ["protocolo", "procedimento", "fluxo", "etapa", "passo"]
        mechanism_keywords = ["mecanismo", "a√ß√£o", "farmacologia", "absor√ß√£o", "metabolismo"]
        
        # Verificar keywords cr√≠ticas primeiro
        if any(keyword in text_lower for keyword in dosage_keywords):
            return "dosage"
        elif any(keyword in text_lower for keyword in safety_keywords):
            return "safety"
        elif any(keyword in text_lower for keyword in protocol_keywords):
            return "protocol"
        elif any(keyword in text_lower for keyword in mechanism_keywords):
            return "mechanism"
        
        # Usar categoria base se n√£o houver match espec√≠fico
        return base_category
    
    def _adjust_priority_by_content(self, text: str, base_priority: float) -> float:
        """Ajusta prioridade baseada no conte√∫do do texto"""
        text_lower = text.lower()
        
        # Aumentar prioridade para conte√∫do cr√≠tico
        critical_terms = ["dosagem", "dose", "contraindica√ß√£o", "perigo", "aten√ß√£o"]
        if any(term in text_lower for term in critical_terms):
            return min(1.0, base_priority + 0.1)
        
        # Diminuir prioridade para conte√∫do geral
        general_terms = ["informa√ß√£o", "geral", "observa√ß√£o"]
        if any(term in text_lower for term in general_terms):
            return max(0.3, base_priority - 0.1)
        
        return base_priority
    
    def generate_document_id(self, text: str, source_file: str) -> str:
        """Gera ID √∫nico para documento"""
        content = f"{source_file}:{text[:100]}"
        # SHA-256 para IDs de documento (n√£o dados sens√≠veis)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def migrate_file(self, filename: str) -> bool:
        """Migra um arquivo JSON espec√≠fico"""
        logger.info(f"üîÑ Migrando arquivo: {filename}")
        
        # Carregar dados
        json_data = self.load_json_file(filename)
        if not json_data:
            return False
        
        file_info = self.json_files[filename]
        
        # Extrair chunks
        chunks = self.extract_chunks_from_json(json_data, file_info)
        if not chunks:
            logger.warning(f"[WARNING] Nenhum chunk extra√≠do de {filename}")
            return False
        
        # Processar chunks e criar documentos
        success_count = 0
        
        for chunk in chunks:
            try:
                # Gerar ID √∫nico
                doc_id = self.generate_document_id(chunk["text"], filename)
                
                # Gerar embedding para o texto
                embedding = None
                if self.embedding_service and self.embedding_service.is_available():
                    embedding = self.embedding_service.embed_text(chunk["text"])
                    if embedding is not None:
                        self.migration_stats["embeddings_generated"] += 1
                        logger.debug(f"[OK] Embedding gerado para chunk {doc_id[:8]}...")
                    else:
                        logger.warning(f"[WARNING] Falha ao gerar embedding para chunk {doc_id[:8]}...")
                else:
                    logger.warning("[WARNING] Servi√ßo de embeddings n√£o dispon√≠vel")
                
                # Criar documento vetorial com embedding gerado
                vector_doc = VectorDocument(
                    id=doc_id,
                    text=chunk["text"],
                    embedding=embedding,
                    metadata=chunk["metadata"],
                    chunk_type=chunk["chunk_type"],
                    priority=chunk["priority"],
                    source_file=filename,
                    created_at=datetime.now(timezone.utc)
                )
                
                # Adicionar ao vector store
                if self.vector_store.add_document(vector_doc):
                    success_count += 1
                    self.migration_stats["documents_indexed"] += 1
                else:
                    logger.warning(f"[WARNING] Falha ao adicionar documento {doc_id[:8]} ao vector store")
                
            except Exception as e:
                error_msg = f"Erro ao processar chunk de {filename}: {e}"
                logger.error(error_msg)
                self.migration_stats["errors"].append(error_msg)
        
        # Estat√≠sticas do arquivo
        self.migration_stats["files_processed"] += 1
        self.migration_stats["chunks_created"] += len(chunks)
        
        logger.info(f"[OK] {filename} migrado: {success_count}/{len(chunks)} chunks processados")
        return success_count > 0
    
    def migrate_all_files(self) -> bool:
        """Migra todos os arquivos JSON"""
        logger.info("[START] Iniciando migra√ß√£o completa dos arquivos JSON")
        
        if not self.validate_environment():
            return False
        
        # Processar cada arquivo
        failed_files = []
        
        for filename in self.json_files.keys():
            logger.info(f"\n{'='*60}")
            logger.info(f"PROCESSANDO: {filename}")
            logger.info(f"{'='*60}")
            
            if not self.migrate_file(filename):
                failed_files.append(filename)
        
        # Estat√≠sticas finais
        self.migration_stats["end_time"] = datetime.now()
        duration = self.migration_stats["end_time"] - self.migration_stats["start_time"]
        
        logger.info(f"\n{'='*60}")
        logger.info("[REPORT] MIGRA√á√ÉO CONCLU√çDA")
        logger.info(f"{'='*60}")
        logger.info(f"[OK] Arquivos processados: {self.migration_stats['files_processed']}")
        logger.info(f"[NOTE] Chunks criados: {self.migration_stats['chunks_created']}")
        logger.info(f"üß† Embeddings gerados: {self.migration_stats['embeddings_generated']}")
        logger.info(f"[SAVE] Documentos indexados: {self.migration_stats['documents_indexed']}")
        logger.info(f"‚è±Ô∏è Dura√ß√£o: {duration}")
        logger.info(f"[ERROR] Erros: {len(self.migration_stats['errors'])}")
        
        if failed_files:
            logger.error(f"[ERROR] Arquivos com falhas: {failed_files}")
        
        if self.migration_stats["errors"]:
            logger.error("[ERROR] Lista de erros:")
            for error in self.migration_stats["errors"]:
                logger.error(f"   - {error}")
        
        # Salvar relat√≥rio
        self.save_migration_report()
        
        return len(failed_files) == 0
    
    def save_migration_report(self):
        """Salva relat√≥rio detalhado da migra√ß√£o"""
        report_path = Path(__file__).parent / f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            report_data = {
                "migration_stats": {
                    **self.migration_stats,
                    "start_time": self.migration_stats["start_time"].isoformat(),
                    "end_time": self.migration_stats["end_time"].isoformat() if self.migration_stats["end_time"] else None
                },
                "files_config": self.json_files,
                "environment": {
                    "supabase_url": self.config.SUPABASE_URL,
                    "vector_db_type": self.config.VECTOR_DB_TYPE,
                    "embedding_model": self.config.EMBEDDING_MODEL,
                    "pgvector_dimensions": self.config.PGVECTOR_DIMENSIONS
                }
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"üìÑ Relat√≥rio salvo: {report_path}")
            
        except Exception as e:
            logger.error(f"Erro ao salvar relat√≥rio: {e}")
    
    def test_vector_search(self, query: str = "dosagem rifampicina") -> bool:
        """Testa busca vetorial ap√≥s migra√ß√£o"""
        logger.info(f"[TEST] Testando busca vetorial: '{query}'")
        
        try:
            # Teste b√°sico de busca
            # Nota: Implementar quando embedding_service estiver dispon√≠vel
            
            # Por enquanto, apenas verificar se documentos foram inseridos
            stats = self.vector_store.get_stats()
            
            logger.info(f"[REPORT] Estat√≠sticas do vector store:")
            logger.info(f"   - Backend: {stats.get('backend', 'unknown')}")
            logger.info(f"   - Documentos: {stats.get('supabase_documents', 0)}")
            logger.info(f"   - Conectado: {stats.get('supabase_connected', False)}")
            
            return stats.get('supabase_documents', 0) > 0
            
        except Exception as e:
            logger.error(f"Erro no teste de busca: {e}")
            return False

def main():
    """Fun√ß√£o principal"""
    print("[START] MIGRA√á√ÉO JSON PARA SUPABASE - FASE 3 RAG")
    print("=" * 60)
    
    migrator = JSONToSupabaseMigrator()
    
    # Executar migra√ß√£o
    if migrator.migrate_all_files():
        print("[OK] MIGRA√á√ÉO CONCLU√çDA COM SUCESSO!")
        
        # Teste de valida√ß√£o
        if migrator.test_vector_search():
            print("[OK] Teste de busca vetorial: PASSOU")
        else:
            print("[WARNING] Teste de busca vetorial: FALHOU (verificar logs)")
        
        return 0
    else:
        print("[ERROR] MIGRA√á√ÉO FALHOU - verificar logs de erro")
        return 1

if __name__ == "__main__":
    exit(main())