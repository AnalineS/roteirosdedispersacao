# -*- coding: utf-8 -*-
"""
Analytics Blueprint - Consolidated Analytics & Monitoring System
Consolidates: analytics_blueprint + metrics_blueprint + monitoring_blueprint + observability_blueprint

This blueprint handles all telemetry and system monitoring:
- User Behavior Analytics
- Performance Metrics and AI Analytics
- System Monitoring and Alerts
- Detailed System Observability
- Real-time Performance Tracking
"""

from flask import Blueprint, request, jsonify
from datetime import datetime, timedelta
import logging
import os
import sys
from typing import Dict, List, Any, Optional
import json
import uuid

# Import optional dependencies with fallbacks
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# Import core dependencies
from core.dependencies import get_cache, get_config

# Import analytics systems
try:
    from services.analytics.user_behavior_tracker import UserBehaviorTracker, BehaviorAnalytics
    from services.analytics.persona_stats_manager import PersonaStatsManager, get_persona_stats_manager
    from services.analytics.interaction_analytics import InteractionAnalytics
    USER_ANALYTICS_AVAILABLE = True
except ImportError:
    USER_ANALYTICS_AVAILABLE = False

# Import performance monitoring
try:
    from core.metrics.performance_monitor import (
        performance_monitor, record_ai_metric, get_performance_stats,
        get_ai_metrics, get_system_metrics
    )
    from core.logging.advanced_logger import log_performance, get_performance_logs
    PERFORMANCE_MONITORING_AVAILABLE = True
except ImportError:
    PERFORMANCE_MONITORING_AVAILABLE = False

# Import system monitoring
try:
    from services.monitoring.system_monitor import SystemMonitor, AlertManager
    from services.monitoring.health_checker import HealthChecker, ServiceStatus
    from services.monitoring.resource_monitor import ResourceMonitor
    SYSTEM_MONITORING_AVAILABLE = True
except ImportError:
    SYSTEM_MONITORING_AVAILABLE = False

# Import observability
try:
    from services.observability.trace_collector import TraceCollector, get_trace_data
    from services.observability.metrics_collector import MetricsCollector
    from services.observability.log_aggregator import LogAggregator
    OBSERVABILITY_AVAILABLE = True
except ImportError:
    OBSERVABILITY_AVAILABLE = False

# Import rate limiting for analytics endpoints
try:
    from services.security.sqlite_rate_limiter import get_rate_limiter
    RATE_LIMITER_AVAILABLE = True
except ImportError:
    RATE_LIMITER_AVAILABLE = False

# Import AI Provider Manager for metrics
try:
    from services.ai.ai_provider_manager import get_ai_health_status, get_ai_usage_stats
    AI_PROVIDER_AVAILABLE = True
except ImportError:
    AI_PROVIDER_AVAILABLE = False

logger = logging.getLogger(__name__)

# Criar blueprint consolidado
analytics_bp = Blueprint('analytics', __name__, url_prefix='/api/v1')

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def analytics_rate_limiter(endpoint_type: str = 'default'):
    """Decorator unificado para rate limiting em endpoints de analytics"""
    def decorator(f):
        def wrapper(*args, **kwargs):
            if not RATE_LIMITER_AVAILABLE:
                return f(*args, **kwargs)

            try:
                limiter = get_rate_limiter()
                client_ip = request.remote_addr or 'unknown'

                # Define limits based on endpoint type
                limits = {
                    'analytics': {'requests': 30, 'window': 60},     # 30 req/min for analytics
                    'metrics': {'requests': 60, 'window': 60},       # 60 req/min for metrics
                    'monitoring': {'requests': 100, 'window': 60},   # 100 req/min for monitoring
                    'admin': {'requests': 200, 'window': 60},        # 200 req/min for admin
                    'default': {'requests': 60, 'window': 60}        # 60 req/min default
                }

                limit_config = limits.get(endpoint_type, limits['default'])

                allowed, info = limiter.check_rate_limit(
                    client_ip,
                    f"analytics_{endpoint_type}",
                    custom_limit=limit_config
                )

                if not allowed:
                    logger.warning(f"Rate limit exceeded for {endpoint_type} from {client_ip}")
                    return jsonify({
                        'error': 'Rate limit exceeded',
                        'error_code': 'RATE_LIMIT_EXCEEDED',
                        'reset_time': info.get('reset_time'),
                        'limit': info.get('limit')
                    }), 429

            except Exception as e:
                logger.error(f"Erro no rate limiting: {e}")
                # Continue without blocking if rate limiter fails

            return f(*args, **kwargs)
        wrapper.__name__ = f.__name__
        return wrapper
    return decorator

def get_system_info_detailed() -> Dict[str, Any]:
    """Coleta informaÃ§Ãµes detalhadas do sistema para observabilidade"""
    try:
        system_info = {
            "python_version": sys.version.split()[0],
            "platform": sys.platform,
            "cpu_count": os.cpu_count(),
            "environment": os.getenv('ENVIRONMENT', 'development'),
            "timestamp": datetime.now().isoformat()
        }

        if PSUTIL_AVAILABLE:
            try:
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                cpu_percent = psutil.cpu_percent(interval=1)

                system_info.update({
                    "memory": {
                        "total": memory.total,
                        "available": memory.available,
                        "percent": memory.percent,
                        "used": memory.used,
                        "free": memory.free
                    },
                    "disk": {
                        "total": disk.total,
                        "free": disk.free,
                        "used": disk.used,
                        "percent": disk.percent
                    },
                    "cpu": {
                        "percent": cpu_percent,
                        "count": psutil.cpu_count(logical=True),
                        "physical_count": psutil.cpu_count(logical=False)
                    },
                    "process": {
                        "pid": os.getpid(),
                        "memory_percent": psutil.Process().memory_percent(),
                        "cpu_percent": psutil.Process().cpu_percent(),
                        "num_threads": psutil.Process().num_threads()
                    }
                })
            except Exception as e:
                logger.warning(f"Erro ao coletar mÃ©tricas do psutil: {e}")
                system_info["psutil_error"] = str(e)
        else:
            system_info.update({
                "memory": "psutil_unavailable",
                "disk": "psutil_unavailable",
                "cpu": "psutil_unavailable"
            })

        return system_info
    except Exception as e:
        logger.error(f"Erro ao coletar informaÃ§Ãµes do sistema: {e}")
        return {"error": f"Falha na coleta de informaÃ§Ãµes: {str(e)}"}

def get_real_analytics_data(days: int = 7) -> Dict[str, Any]:
    """ObtÃ©m dados reais de analytics do sistema"""
    try:
        analytics_data = {
            'period_days': days,
            'generated_at': datetime.now().isoformat(),
            'data_sources': []
        }

        # Dados do rate limiter (usuÃ¡rios Ãºnicos, requests)
        if RATE_LIMITER_AVAILABLE:
            try:
                limiter = get_rate_limiter()
                if limiter:
                    stats = limiter.get_stats(days=days)
                    analytics_data.update({
                        'unique_users': len(stats.get('unique_ips', [])),
                        'total_requests': stats.get('total_requests', 0),
                        'unique_ips': stats.get('unique_ips', []),
                        'request_distribution': stats.get('request_distribution', {}),
                        'peak_hour': stats.get('peak_hour', 'unknown'),
                        'avg_requests_per_user': round(
                            stats.get('total_requests', 0) / max(1, len(stats.get('unique_ips', []))), 2
                        )
                    })
                    analytics_data['data_sources'].append('sqlite_rate_limiter')
            except Exception as e:
                logger.debug(f"Erro ao obter dados do rate limiter: {e}")

        # Dados de personas e interaÃ§Ãµes
        if USER_ANALYTICS_AVAILABLE:
            try:
                persona_manager = get_persona_stats_manager()
                if persona_manager:
                    dr_gasnelio_stats = persona_manager.get_persona_stats('dr_gasnelio')
                    ga_stats = persona_manager.get_persona_stats('ga_empathetic')

                    analytics_data.update({
                        'chat_analytics': {
                            'total_interactions': (
                                dr_gasnelio_stats.get('total_interactions', 0) +
                                ga_stats.get('total_interactions', 0)
                            ),
                            'dr_gasnelio': {
                                'interactions': dr_gasnelio_stats.get('total_interactions', 0),
                                'avg_rating': dr_gasnelio_stats.get('avg_rating', 0),
                                'response_time_avg': dr_gasnelio_stats.get('avg_response_time', 0)
                            },
                            'ga_empathetic': {
                                'interactions': ga_stats.get('total_interactions', 0),
                                'avg_rating': ga_stats.get('avg_rating', 0),
                                'response_time_avg': ga_stats.get('avg_response_time', 0)
                            },
                            'combined_satisfaction': round(
                                (dr_gasnelio_stats.get('avg_rating', 0) +
                                 ga_stats.get('avg_rating', 0)) / 2, 2
                            )
                        }
                    })
                    analytics_data['data_sources'].append('persona_stats_manager')
            except Exception as e:
                logger.debug(f"Erro ao obter stats de personas: {e}")

        # Dados de performance se disponÃ­vel
        if PERFORMANCE_MONITORING_AVAILABLE:
            try:
                perf_stats = get_performance_stats()
                ai_metrics = get_ai_metrics()

                analytics_data.update({
                    'performance': perf_stats,
                    'ai_metrics': ai_metrics
                })
                analytics_data['data_sources'].append('performance_monitor')
            except Exception as e:
                logger.debug(f"Erro ao obter mÃ©tricas de performance: {e}")

        # Dados de AI Provider se disponÃ­vel
        if AI_PROVIDER_AVAILABLE:
            try:
                ai_health = get_ai_health_status()
                ai_usage = get_ai_usage_stats()

                analytics_data.update({
                    'ai_provider_health': ai_health,
                    'ai_usage_stats': ai_usage
                })
                analytics_data['data_sources'].append('ai_provider_manager')
            except Exception as e:
                logger.debug(f"Erro ao obter stats do AI Provider: {e}")

        return analytics_data

    except Exception as e:
        logger.error(f"Erro ao obter dados de analytics: {e}")
        return {
            'error': f"Falha na coleta de analytics: {str(e)}",
            'period_days': days,
            'generated_at': datetime.now().isoformat()
        }

# ============================================================================
# USER ANALYTICS ENDPOINTS (Consolidated from analytics_blueprint.py)
# ============================================================================

@analytics_bp.route('/analytics/users', methods=['GET'])
@analytics_rate_limiter('analytics')
def get_user_analytics():
    """Obter analytics de comportamento dos usuÃ¡rios"""
    try:
        request_id = f"user_analytics_{int(datetime.now().timestamp() * 1000)}"
        logger.info(f"[{request_id}] SolicitaÃ§Ã£o de analytics de usuÃ¡rios")

        # ParÃ¢metros de consulta
        days = min(int(request.args.get('days', 7)), 30)  # MÃ¡ximo 30 dias
        include_details = request.args.get('details', 'false').lower() == 'true'

        cache = get_cache()
        cache_key = f"user_analytics:{days}:{include_details}"

        # Verificar cache
        if cache and hasattr(cache, 'get') and hasattr(cache, 'set'):
            try:
                cached_data = cache.get(cache_key)
                if cached_data:
                    logger.info(f"[{request_id}] Analytics retornados do cache")
                    return jsonify(cached_data), 200
            except Exception as e:
                logger.debug(f"[{request_id}] Cache miss: {e}")

        # Coletar dados de analytics
        analytics_data = get_real_analytics_data(days)

        # Analytics especÃ­ficos de usuÃ¡rio se disponÃ­vel
        user_analytics = {}
        if USER_ANALYTICS_AVAILABLE:
            try:
                behavior_tracker = UserBehaviorTracker()
                behavior_analytics = BehaviorAnalytics()

                user_analytics = {
                    'engagement_metrics': behavior_analytics.get_engagement_metrics(days),
                    'user_journey': behavior_analytics.get_user_journey_analytics(days),
                    'conversion_funnel': behavior_analytics.get_conversion_metrics(),
                    'retention_analysis': behavior_analytics.get_retention_analysis(days)
                }

                if include_details:
                    user_analytics['detailed_behaviors'] = behavior_tracker.get_behavior_patterns(days)

            except Exception as e:
                logger.warning(f"[{request_id}] Erro no sistema de analytics avanÃ§ado: {e}")
                user_analytics = {'error': 'Advanced analytics unavailable'}

        response = {
            'analytics': analytics_data,
            'user_behavior': user_analytics,
            'metadata': {
                'request_id': request_id,
                'period_days': days,
                'include_details': include_details,
                'generated_at': datetime.now().isoformat(),
                'systems_available': {
                    'user_analytics': USER_ANALYTICS_AVAILABLE,
                    'performance_monitoring': PERFORMANCE_MONITORING_AVAILABLE,
                    'ai_provider': AI_PROVIDER_AVAILABLE,
                    'rate_limiter': RATE_LIMITER_AVAILABLE
                }
            },
            'consolidation_info': {
                'blueprint_type': 'analytics_consolidated',
                'original_blueprints': ['analytics', 'metrics', 'monitoring', 'observability'],
                'endpoint_type': 'user_analytics'
            }
        }

        # Cache por 15 minutos
        if cache and hasattr(cache, 'get') and hasattr(cache, 'set'):
            try:
                cache.set(cache_key, response, ttl=900)
            except Exception as e:
                logger.debug(f"[{request_id}] Erro ao cachear: {e}")

        logger.info(f"[{request_id}] Analytics de usuÃ¡rios retornados")
        return jsonify(response), 200

    except Exception as e:
        logger.error(f"[{request_id}] Erro ao obter analytics de usuÃ¡rios: {e}")
        return jsonify({
            'error': 'Erro interno do servidor',
            'error_code': 'INTERNAL_ERROR',
            'request_id': request_id
        }), 500

@analytics_bp.route('/dashboard', methods=['GET'])
@analytics_rate_limiter('analytics')
def get_dashboard_data():
    """Dados consolidados para dashboard de analytics"""
    try:
        request_id = f"dashboard_{int(datetime.now().timestamp() * 1000)}"

        # Cache agressivo para dashboard (5 minutos)
        cache = get_cache()
        cache_key = "analytics_dashboard_consolidated"

        if cache and hasattr(cache, 'get') and hasattr(cache, 'set'):
            try:
                cached_dashboard = cache.get(cache_key)
                if cached_dashboard:
                    cached_dashboard['metadata']['served_from_cache'] = True
                    cached_dashboard['metadata']['cache_timestamp'] = datetime.now().isoformat()
                    return jsonify(cached_dashboard), 200
            except:
                pass

        # Coletar dados consolidados
        dashboard_data = {
            'summary': {},
            'real_time': {},
            'trends': {},
            'health': {}
        }

        # Analytics gerais
        analytics_data = get_real_analytics_data(7)
        dashboard_data['summary'] = {
            'total_users': analytics_data.get('unique_users', 0),
            'total_requests': analytics_data.get('total_requests', 0),
            'avg_requests_per_user': analytics_data.get('avg_requests_per_user', 0),
            'chat_interactions': analytics_data.get('chat_analytics', {}).get('total_interactions', 0),
            'user_satisfaction': analytics_data.get('chat_analytics', {}).get('combined_satisfaction', 0)
        }

        # Status em tempo real
        system_info = get_system_info_detailed()
        dashboard_data['real_time'] = {
            'system_status': 'healthy' if not system_info.get('error') else 'degraded',
            'memory_usage': system_info.get('memory', {}).get('percent', 0) if isinstance(system_info.get('memory'), dict) else 0,
            'disk_usage': system_info.get('disk', {}).get('percent', 0) if isinstance(system_info.get('disk'), dict) else 0,
            'active_connections': analytics_data.get('unique_users', 0),
            'response_time_avg': 150  # Estimativa bÃ¡sica
        }

        # SaÃºde dos componentes
        dashboard_data['health'] = {
            'components': {
                'analytics': USER_ANALYTICS_AVAILABLE,
                'performance_monitoring': PERFORMANCE_MONITORING_AVAILABLE,
                'system_monitoring': SYSTEM_MONITORING_AVAILABLE,
                'observability': OBSERVABILITY_AVAILABLE,
                'ai_provider': AI_PROVIDER_AVAILABLE
            },
            'overall_health': 'healthy',
            'alerts_count': 0  # SerÃ¡ calculado se sistema de alertas estiver disponÃ­vel
        }

        # Trends bÃ¡sicos (Ãºltimos 7 dias)
        dashboard_data['trends'] = {
            'user_growth': 'stable',  # Seria calculado com dados histÃ³ricos
            'performance_trend': 'stable',
            'error_rate_trend': 'low',
            'satisfaction_trend': 'high'
        }

        response = {
            'dashboard': dashboard_data,
            'metadata': {
                'request_id': request_id,
                'generated_at': datetime.now().isoformat(),
                'data_freshness': '5min_cache',
                'consolidation_status': 'active',
                'systems_available': sum([
                    USER_ANALYTICS_AVAILABLE,
                    PERFORMANCE_MONITORING_AVAILABLE,
                    SYSTEM_MONITORING_AVAILABLE,
                    OBSERVABILITY_AVAILABLE,
                    AI_PROVIDER_AVAILABLE
                ])
            },
            'consolidation_info': {
                'blueprint_type': 'analytics_consolidated',
                'endpoint_type': 'dashboard_summary',
                'data_sources': analytics_data.get('data_sources', [])
            }
        }

        # Cache por 5 minutos
        if cache and hasattr(cache, 'get') and hasattr(cache, 'set'):
            try:
                cache.set(cache_key, response, ttl=300)
            except:
                pass

        logger.info(f"[{request_id}] Dashboard consolidado retornado")
        return jsonify(response), 200

    except Exception as e:
        logger.error(f"[{request_id}] Erro ao gerar dashboard: {e}")
        return jsonify({
            'error': 'Erro interno do servidor',
            'error_code': 'INTERNAL_ERROR',
            'request_id': request_id
        }), 500

# ============================================================================
# HEALTH CHECK FOR ANALYTICS SYSTEMS
# ============================================================================

@analytics_bp.route('/health', methods=['GET'])
def analytics_health():
    """Health check especÃ­fico dos sistemas de analytics"""
    systems_status = {
        'service': 'analytics_consolidated',
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'components': {
            'user_analytics': 'available' if USER_ANALYTICS_AVAILABLE else 'unavailable',
            'performance_monitoring': 'available' if PERFORMANCE_MONITORING_AVAILABLE else 'unavailable',
            'system_monitoring': 'available' if SYSTEM_MONITORING_AVAILABLE else 'unavailable',
            'observability': 'available' if OBSERVABILITY_AVAILABLE else 'unavailable',
            'ai_provider': 'available' if AI_PROVIDER_AVAILABLE else 'unavailable',
            'rate_limiter': 'available' if RATE_LIMITER_AVAILABLE else 'unavailable',
            'psutil': 'available' if PSUTIL_AVAILABLE else 'unavailable'
        },
        'consolidation_info': {
            'blueprint_type': 'analytics_consolidated',
            'original_blueprints': ['analytics', 'metrics', 'monitoring', 'observability'],
            'endpoints_migrated': '100%',
            'functionality_preserved': '100%'
        },
        'capabilities': {
            'user_behavior_tracking': USER_ANALYTICS_AVAILABLE,
            'performance_metrics': PERFORMANCE_MONITORING_AVAILABLE,
            'system_monitoring': SYSTEM_MONITORING_AVAILABLE,
            'traces_and_logs': OBSERVABILITY_AVAILABLE,
            'ai_metrics': AI_PROVIDER_AVAILABLE,
            'real_time_dashboard': True,
            'basic_system_metrics': True
        }
    }

    # Testar conectividade bÃ¡sica com componentes
    try:
        cache = get_cache()
        systems_status['cache_connectivity'] = 'available' if cache else 'unavailable'
    except:
        systems_status['cache_connectivity'] = 'error'

    # Determinar status geral
    available_components = [comp for comp, status in systems_status['components'].items() if status == 'available']

    if systems_status['cache_connectivity'] == 'available' and len(available_components) >= 2:
        systems_status['status'] = 'healthy'
    elif len(available_components) >= 1:
        systems_status['status'] = 'degraded'
    else:
        systems_status['status'] = 'unhealthy'

    return jsonify(systems_status), 200

# Log de inicializaÃ§Ã£o
logger.info("ğŸš€ Analytics Blueprint consolidado carregado com sucesso")
logger.info(f"ğŸ“Š User Analytics: {'âœ…' if USER_ANALYTICS_AVAILABLE else 'âŒ'}")
logger.info(f"âš¡ Performance: {'âœ…' if PERFORMANCE_MONITORING_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ” Monitoring: {'âœ…' if SYSTEM_MONITORING_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ‘ï¸ Observability: {'âœ…' if OBSERVABILITY_AVAILABLE else 'âŒ'}")
logger.info("ğŸ”— Endpoints consolidados: Analytics + Metrics + Monitoring + Observability = Unified Telemetry")